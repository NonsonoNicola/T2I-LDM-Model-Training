{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f57dc15a-c9e4-4ad3-b048-b92fea5e64d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: diffusers in /usr/local/lib/python3.11/dist-packages (0.33.1)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.6.0)\n",
      "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.20.1)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (11.2.1)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
      "Requirement already satisfied: xformers in /usr/local/lib/python3.11/dist-packages (0.0.30)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.7.0)\n",
      "Requirement already satisfied: importlib-metadata in /usr/lib/python3/dist-packages (from diffusers) (4.6.4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from diffusers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.27.0 in /usr/local/lib/python3.11/dist-packages (from diffusers) (0.33.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from diffusers) (2.1.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from diffusers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from diffusers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from diffusers) (0.5.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.3.0)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2024.10.0)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.2.1)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n",
      "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.7)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (6.31.1)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (7.0.0)\n",
      "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.5)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.30.0)\n",
      "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.6)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.8 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.12.2)\n",
      "Requirement already satisfied: torch==2.7.0 in /usr/local/lib/python3.11/dist-packages (from xformers) (2.7.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->xformers) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->xformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->xformers) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->xformers) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->xformers) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->xformers) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->xformers) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->xformers) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->xformers) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->xformers) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->xformers) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->xformers) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->xformers) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->xformers) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->xformers) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->xformers) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->xformers) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->xformers) (3.3.0)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.3.0->torch==2.7.0->xformers) (77.0.1)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.12)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.27.0->diffusers) (1.1.3)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->diffusers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->diffusers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->diffusers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->diffusers) (2025.1.31)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch==2.7.0->xformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.7.0->xformers) (2.1.5)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install diffusers transformers datasets wandb Pillow tqdm xformers accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa4c95c4-a7b2-4d09-b760-46686fd485d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from huggingface_hub import login\n",
    "#login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2cfe31d-2451-4d8d-bffe-9bf52b29ec82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1cfa9de-7fe4-48d9-9558-9899ed5c6401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading VAE...\n",
      "Loading Text Encoder & Tokenizer...\n",
      "Initializing UNet...\n",
      "UNet initialized with 441.92M parameters.\n",
      "XFormers memory efficient attention enabled for UNet.\n",
      "Skipping UNet compilation to avoid potential CUDAGraph issues.\n",
      "Setting up dataset stream...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e29e82f1b174fa7ae9ab311c90a5316",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/1831 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found checkpoint: /workspace/PozzyDiffusion/checkpoints/ckpt_step300\n",
      "EMA UNet state loaded.\n",
      "Resumed training from step 300, epoch 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnonsononicola\u001b[0m (\u001b[33mnonsononicola-individual\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/wandb/run-20250613_114159-oqaw8w8l</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Resuming run <strong><a href='https://wandb.ai/nonsononicola-individual/PozzyDiffusion_A40/runs/oqaw8w8l' target=\"_blank\">wandering-glitter-36</a></strong> to <a href='https://wandb.ai/nonsononicola-individual/PozzyDiffusion_A40' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nonsononicola-individual/PozzyDiffusion_A40' target=\"_blank\">https://wandb.ai/nonsononicola-individual/PozzyDiffusion_A40</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nonsononicola-individual/PozzyDiffusion_A40/runs/oqaw8w8l' target=\"_blank\">https://wandb.ai/nonsononicola-individual/PozzyDiffusion_A40/runs/oqaw8w8l</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming WandB run with ID: oqaw8w8l\n",
      "Starting training. Target steps: 30000. Current step: 300. Warmup steps: 1500.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6456bc489523431fbfa815df318c0caf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Steps:   1%|1         | 300/30000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 12691a8f-4541-464d-aa03-9c2591d79dc9)')' thrown while requesting GET https://huggingface.co/datasets/BLIP3o/BLIP3o-Pretrain-Short-Caption/resolve/e84d184540a6cc4fc3b1ba5528bb45dcc6b3fb0e/00001.tar\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: c629f7c7-ad9c-466a-9c98-66a6b9924de2)')' thrown while requesting GET https://huggingface.co/datasets/BLIP3o/BLIP3o-Pretrain-Short-Caption/resolve/e84d184540a6cc4fc3b1ba5528bb45dcc6b3fb0e/00001.tar\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 595c43e1-5e79-4149-8b5d-f97b67f31b16)')' thrown while requesting GET https://huggingface.co/datasets/BLIP3o/BLIP3o-Pretrain-Short-Caption/resolve/e84d184540a6cc4fc3b1ba5528bb45dcc6b3fb0e/00007.tar\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: c66aefd2-3b1f-4d95-9527-9dbccb9f92ba)')' thrown while requesting GET https://huggingface.co/datasets/BLIP3o/BLIP3o-Pretrain-Short-Caption/resolve/e84d184540a6cc4fc3b1ba5528bb45dcc6b3fb0e/00007.tar\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: fe4040a4-7871-4943-9678-0b9ca0e32253)')' thrown while requesting GET https://huggingface.co/datasets/BLIP3o/BLIP3o-Pretrain-Short-Caption/resolve/e84d184540a6cc4fc3b1ba5528bb45dcc6b3fb0e/00002.tar\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'(ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')), '(Request ID: c0bbde95-9f90-4700-8dd4-31311a396b6d)')' thrown while requesting GET https://huggingface.co/datasets/BLIP3o/BLIP3o-Pretrain-Short-Caption/resolve/e84d184540a6cc4fc3b1ba5528bb45dcc6b3fb0e/00007.tar\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'(ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')), '(Request ID: a02584d3-f092-4c7a-9ae6-dd4ef745443b)')' thrown while requesting GET https://huggingface.co/datasets/BLIP3o/BLIP3o-Pretrain-Short-Caption/resolve/e84d184540a6cc4fc3b1ba5528bb45dcc6b3fb0e/00004.tar\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'(ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')), '(Request ID: ab6c47e5-d8f4-483d-aef6-a0712ea209ba)')' thrown while requesting GET https://huggingface.co/datasets/BLIP3o/BLIP3o-Pretrain-Short-Caption/resolve/e84d184540a6cc4fc3b1ba5528bb45dcc6b3fb0e/00005.tar\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'(ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')), '(Request ID: 1d47c38a-2bd9-421c-98df-0c17b44bbc59)')' thrown while requesting GET https://huggingface.co/datasets/BLIP3o/BLIP3o-Pretrain-Short-Caption/resolve/e84d184540a6cc4fc3b1ba5528bb45dcc6b3fb0e/00012.tar\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'(ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')), '(Request ID: 4ee25354-853a-4742-af7e-042c5b2f6b95)')' thrown while requesting GET https://huggingface.co/datasets/BLIP3o/BLIP3o-Pretrain-Short-Caption/resolve/e84d184540a6cc4fc3b1ba5528bb45dcc6b3fb0e/00015.tar\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'(ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')), '(Request ID: 95418ab7-308f-47f0-8de5-803e324c43de)')' thrown while requesting GET https://huggingface.co/datasets/BLIP3o/BLIP3o-Pretrain-Short-Caption/resolve/e84d184540a6cc4fc3b1ba5528bb45dcc6b3fb0e/00010.tar\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'(ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')), '(Request ID: 07c8cba0-f57e-4a69-88f1-bd4292245099)')' thrown while requesting GET https://huggingface.co/datasets/BLIP3o/BLIP3o-Pretrain-Short-Caption/resolve/e84d184540a6cc4fc3b1ba5528bb45dcc6b3fb0e/00008.tar\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'(ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')), '(Request ID: 67fb789c-f86b-4216-aa51-f668f92080fc)')' thrown while requesting GET https://huggingface.co/datasets/BLIP3o/BLIP3o-Pretrain-Short-Caption/resolve/e84d184540a6cc4fc3b1ba5528bb45dcc6b3fb0e/00011.tar\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'(ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')), '(Request ID: 96341984-01ba-4c66-8fa1-d71cd5ad7850)')' thrown while requesting GET https://huggingface.co/datasets/BLIP3o/BLIP3o-Pretrain-Short-Caption/resolve/e84d184540a6cc4fc3b1ba5528bb45dcc6b3fb0e/00014.tar\n",
      "Retrying in 1s [Retry 1/5].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint: /workspace/PozzyDiffusion/checkpoints/ckpt_step600\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5078cd2412a543ff84dc524df08eab5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling for prompt 1:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated sample for prompt 'a photo of an astronaut riding a horse on mars' saved to /workspace/PozzyDiffusion/samples/step_600/prompt_1_seed42.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd32d60b1fa545baaf5001c4b73604fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling for prompt 2:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated sample for prompt 'a fantasy landscape with a castle and a dragon' saved to /workspace/PozzyDiffusion/samples/step_600/prompt_2_seed43.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd3c5768408b4685ac49fc7cc1b5e689",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling for prompt 3:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated sample for prompt 'a cyberpunk city at night with neon lights' saved to /workspace/PozzyDiffusion/samples/step_600/prompt_3_seed44.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0311eeb6b65d4faab1abcdf57dedcf8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling for prompt 4:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated sample for prompt 'a serene Japanese garden with cherry blossoms' saved to /workspace/PozzyDiffusion/samples/step_600/prompt_4_seed45.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da0d73626fc14cdf9f3d996e1c4f4419",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling for prompt 5:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated sample for prompt 'a whimsical illustration of a cat playing a piano' saved to /workspace/PozzyDiffusion/samples/step_600/prompt_5_seed46.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x7764827c53d0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt: \n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import io\n",
    "import glob\n",
    "import shutil\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import math\n",
    "from torch.utils.data import DataLoader, IterableDataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image, ImageOps\n",
    "from tqdm.auto import tqdm\n",
    "import wandb\n",
    "import xformers # Added for XFORMERS\n",
    "\n",
    "# Hugging Face Libraries\n",
    "from diffusers import AutoencoderKL, UNet2DConditionModel, DDPMScheduler\n",
    "from diffusers.training_utils import EMAModel\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from datasets import load_dataset\n",
    "from accelerate import Accelerator, DistributedDataParallelKwargs\n",
    "from accelerate.utils import ProjectConfiguration, set_seed, LoggerType\n",
    "\n",
    "# --- Configuration (Hardcoded for simplicity) ---\n",
    "vae_model_id = \"stabilityai/sd-vae-ft-ema\"\n",
    "text_encoder_model_id = \"openai/clip-vit-large-patch14\"\n",
    "dataset_id = \"BLIP3o/BLIP3o-Pretrain-Short-Caption\"\n",
    "\n",
    "# Training Hyperparameters\n",
    "image_resolution = 512 #Il mio codice nella teoria fa il resize delle immagini in caso si vogliano utilizzare altri DS\n",
    "latent_resolution = image_resolution // 8 #image_resolution // vae_scale_factor (solitamente 8)\n",
    "train_batch_size = 16 # Batch per scheda video\n",
    "gradient_accumulation_steps = 8 # Effective batch size (GPU * acc_steps * batch_size)\n",
    "learning_rate = 1e-4 #Non sapendo bene che fare questa va bene, credo, la chat dice che va bene\n",
    "adam_beta1 = 0.9 #std\n",
    "adam_beta2 = 0.999 #std\n",
    "adam_weight_decay = 1e-2 #std\n",
    "adam_epsilon = 1e-08 #std\n",
    "max_grad_norm = 1.0 #Mi taglio le ve-...i gradienti\n",
    "num_train_timesteps_scheduler = 1000 #Step del noise scheduler\n",
    "mixed_precision = \"bf16\" #Risparmia memoria e va pure piu veloce a fare i calcoli\n",
    "seed = 42 #42 risolve tutto, anche perche questa pila fumante di merda potrebbe funzionare cosi\n",
    "cfg_drop_probability = 0.1 # CFG Drop Probability\n",
    "max_train_steps = 30_000\n",
    "save_every_steps = max_train_steps // 100\n",
    "num_warmup_steps_ratio = 0.05 # 5% of max_train_steps for warmup\n",
    "num_warmup_steps = int(num_warmup_steps_ratio * max_train_steps)\n",
    "# --- Storage Path ---\n",
    "persistent_storage_mount_path = \"/workspace\"\n",
    "project_data_folder_name = \"PozzyDiffusion\"\n",
    "output_dir_base = os.path.join(persistent_storage_mount_path, project_data_folder_name)\n",
    "logging_dir = os.path.join(output_dir_base, \"logs\")\n",
    "checkpoint_dir_base = os.path.join(output_dir_base, \"checkpoints\")\n",
    "final_model_dir = os.path.join(output_dir_base, \"final_model\")\n",
    "max_keep_checkpoints = 2\n",
    "# WandB\n",
    "wandb_project_name = \"PozzyDiffusion_A40\"\n",
    "use_wandb = True\n",
    "# Hardcoded prompts per sampling\n",
    "sampling_prompts = [\n",
    "    \"a photo of an astronaut riding a horse on mars\",\n",
    "    \"a fantasy landscape with a castle and a dragon\",\n",
    "    \"a cyberpunk city at night with neon lights\",\n",
    "    \"a serene Japanese garden with cherry blossoms\",\n",
    "    \"a whimsical illustration of a cat playing a piano\"\n",
    "]\n",
    "num_inference_steps_sampling = 50 #Step che fa per ottenere l'img finale\n",
    "\n",
    "# --- Accelerator ---\n",
    "project_config = ProjectConfiguration(project_dir=output_dir_base, logging_dir=logging_dir)\n",
    "accelerator = Accelerator(\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    mixed_precision=mixed_precision,\n",
    "    log_with=\"wandb\" if use_wandb else None,\n",
    "    project_config=project_config,\n",
    ")\n",
    "# Imposta in modo globale il random seed cosi che posso riprodurre perfettamente tutto\n",
    "if seed is not None:\n",
    "    set_seed(seed)\n",
    "# Crea gia le cartelle\n",
    "if accelerator.is_main_process:\n",
    "    os.makedirs(output_dir_base, exist_ok=True)\n",
    "    os.makedirs(checkpoint_dir_base, exist_ok=True)\n",
    "    os.makedirs(final_model_dir, exist_ok=True)\n",
    "\n",
    "# --- Helper Functions ---\n",
    "def save_checkpoint_custom(accelerator, unet_original, ema_unet, optimizer, lr_scheduler, global_step, epoch, checkpoint_dir_base, filename_prefix=\"ckpt\", max_keep=2):\n",
    "    if accelerator.is_main_process:\n",
    "        os.makedirs(checkpoint_dir_base, exist_ok=True)\n",
    "        # Checkpoint path includes the step\n",
    "        checkpoint_name = f\"{filename_prefix}_step{global_step}\"\n",
    "        save_path = os.path.join(checkpoint_dir_base, checkpoint_name)\n",
    "        \n",
    "        accelerator.save_state(save_path) # Saves prepared models, optimizer, scheduler\n",
    "\n",
    "        # Save EMA model separately as it's not part of accelerator.prepare() directly\n",
    "        ema_save_path = os.path.join(save_path, \"ema_unet.pth\")\n",
    "        torch.save(ema_unet.state_dict(), ema_save_path)\n",
    "        \n",
    "        # Save additional training state (could be registered with accelerator too)\n",
    "        custom_state = {\n",
    "            'global_step': global_step,\n",
    "            'epoch': epoch,\n",
    "            'wandb_run_id': wandb.run.id if use_wandb and wandb.run else None\n",
    "        }\n",
    "        torch.save(custom_state, os.path.join(save_path, \"custom_training_state.pt\"))\n",
    "\n",
    "        accelerator.print(f\"Saved checkpoint: {save_path}\")\n",
    "\n",
    "        # Manage old checkpoints\n",
    "        checkpoints = sorted(\n",
    "            glob.glob(os.path.join(checkpoint_dir_base, f\"{filename_prefix}_step*\")),\n",
    "            key=lambda x: int(os.path.basename(x).split('step')[-1]) # Sort by step number\n",
    "        )\n",
    "        if len(checkpoints) > max_keep:\n",
    "            for old_ckpt_path in checkpoints[:-max_keep]:\n",
    "                accelerator.print(f\"Removing old checkpoint: {old_ckpt_path}\")\n",
    "                shutil.rmtree(old_ckpt_path) # Remove entire directory\n",
    "\n",
    "def load_latest_checkpoint_custom(accelerator, unet_original, ema_unet, optimizer, lr_scheduler, checkpoint_dir_base, filename_prefix=\"ckpt\"):\n",
    "    checkpoints = sorted(\n",
    "        glob.glob(os.path.join(checkpoint_dir_base, f\"{filename_prefix}_step*\")),\n",
    "        key=lambda x: int(os.path.basename(x).split('step')[-1]), # Sort by step number\n",
    "        reverse=True\n",
    "    )\n",
    "    if checkpoints:\n",
    "        latest_checkpoint_path = checkpoints[0]\n",
    "        accelerator.print(f\"Found checkpoint: {latest_checkpoint_path}\")\n",
    "        try:\n",
    "            accelerator.load_state(latest_checkpoint_path)\n",
    "            \n",
    "            ema_path = os.path.join(latest_checkpoint_path, \"ema_unet.pth\")\n",
    "            if os.path.exists(ema_path):\n",
    "                ema_unet.load_state_dict(torch.load(ema_path, map_location=\"cpu\"))\n",
    "                accelerator.print(\"EMA UNet state loaded.\")\n",
    "\n",
    "            custom_state_path = os.path.join(latest_checkpoint_path, \"custom_training_state.pt\")\n",
    "            if os.path.exists(custom_state_path):\n",
    "                custom_state = torch.load(custom_state_path, map_location=\"cpu\")\n",
    "                return custom_state.get('global_step', 0), custom_state.get('epoch', 0), custom_state.get('wandb_run_id')\n",
    "            else: # Fallback for older checkpoints perhaps, or if custom state saving failed\n",
    "                accelerator.print(\"Warning: custom_training_state.pt not found. Global step and epoch might not be accurate from checkpoint.\")\n",
    "                # Try to infer global_step from path if needed, though accelerator might handle optimizer step counts.\n",
    "                # For this setup, accelerator.load_state() should restore optimizer and scheduler internal steps.\n",
    "                # The main global_step is for tracking progress and naming.\n",
    "                parsed_step = int(os.path.basename(latest_checkpoint_path).split('step')[-1])\n",
    "                return parsed_step, 0, None # Assuming epoch 0 if not found\n",
    "\n",
    "        except Exception as e:\n",
    "            accelerator.print(f\"Error loading checkpoint {latest_checkpoint_path}: {e}. Starting fresh or from an earlier state if accelerator handled it.\")\n",
    "            return 0, 0, None # Could not load\n",
    "    return 0, 0, None # No checkpoint found\n",
    "\n",
    "def generate_samples(unet_model_for_sampling, vae, text_encoder, tokenizer_obj, noise_scheduler_obj, prompts, output_dir, global_step, device, accelerator_ref):\n",
    "    unet_model_for_sampling.eval() # Ensure the passed model is set to eval\n",
    "    vae.eval()\n",
    "    text_encoder.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, prompt in enumerate(prompts):\n",
    "            text_inputs = tokenizer_obj(prompt, padding=\"max_length\", max_length=tokenizer_obj.model_max_length, truncation=True, return_tensors=\"pt\")\n",
    "            # Move input_ids to the correct device\n",
    "            text_embeddings = text_encoder(text_inputs.input_ids.to(device))[0]\n",
    "            \n",
    "            # Use accelerator.device for latents\n",
    "            latents_shape = (1, unet_model_for_sampling.config.in_channels, latent_resolution, latent_resolution)\n",
    "            \n",
    "            # --- FIXED LINE ---\n",
    "            # The generator must be created on the same device as the target tensor.\n",
    "            # torch.manual_seed() creates a CPU generator by default.\n",
    "            # The fix is to instantiate a generator on the correct device.\n",
    "            generator = torch.Generator(device=device).manual_seed(seed + i) if seed is not None else None\n",
    "            latents = torch.randn(latents_shape, device=device, generator=generator) # Add seed for reproducibility of samples\n",
    "            \n",
    "            noise_scheduler_obj.set_timesteps(num_inference_steps_sampling) # Use configured number of steps\n",
    "            \n",
    "            for t in tqdm(noise_scheduler_obj.timesteps, desc=f\"Sampling for prompt {i+1}\", disable=not accelerator_ref.is_main_process):\n",
    "                # scale the model input\n",
    "                latent_model_input = noise_scheduler_obj.scale_model_input(latents, t)\n",
    "                noise_pred = unet_model_for_sampling(latent_model_input, t, encoder_hidden_states=text_embeddings).sample\n",
    "                latents = noise_scheduler_obj.step(noise_pred, t, latents).prev_sample\n",
    "            \n",
    "            latents = 1 / vae.config.scaling_factor * latents\n",
    "            image = vae.decode(latents).sample\n",
    "            image = (image / 2 + 0.5).clamp(0, 1) # Denormalize\n",
    "            image = image.cpu().permute(0, 2, 3, 1).numpy()[0] # (H, W, C)\n",
    "            image = Image.fromarray((image * 255).round().astype(\"uint8\"))\n",
    "            \n",
    "            if accelerator_ref.is_main_process:\n",
    "                sample_output_dir = os.path.join(output_dir, \"samples\", f\"step_{global_step}\")\n",
    "                os.makedirs(sample_output_dir, exist_ok=True)\n",
    "                img_path = os.path.join(sample_output_dir, f\"prompt_{i+1}_seed{seed+i if seed is not None else 'rand'}.png\")\n",
    "                image.save(img_path)\n",
    "                accelerator_ref.print(f\"Generated sample for prompt '{prompt}' saved to {img_path}\")\n",
    "    \n",
    "    unet_model_for_sampling.train() # Set back to train if it was the main model (though here it's a copy)\n",
    "\n",
    "# --- Inizializza i modelli ---\n",
    "accelerator.print(\"Loading VAE...\")\n",
    "vae = AutoencoderKL.from_pretrained(vae_model_id)\n",
    "accelerator.print(\"Loading Text Encoder & Tokenizer...\")\n",
    "tokenizer = CLIPTokenizer.from_pretrained(text_encoder_model_id)\n",
    "text_encoder = CLIPTextModel.from_pretrained(text_encoder_model_id)\n",
    "accelerator.print(\"Initializing UNet...\")\n",
    "# Struttura originale su cui si basera l'EMA\n",
    "original_unet = UNet2DConditionModel(\n",
    "    sample_size=latent_resolution, in_channels=vae.config.latent_channels, out_channels=vae.config.latent_channels,\n",
    "    layers_per_block=2, block_out_channels=(256, 512, 768, 1024), # Example, adjust as needed\n",
    "    down_block_types=(\"CrossAttnDownBlock2D\", \"CrossAttnDownBlock2D\", \"CrossAttnDownBlock2D\", \"DownBlock2D\"),\n",
    "    up_block_types=(\"UpBlock2D\", \"CrossAttnUpBlock2D\", \"CrossAttnUpBlock2D\", \"CrossAttnUpBlock2D\"),\n",
    "    cross_attention_dim=text_encoder.config.hidden_size,\n",
    ")\n",
    "#Conto i parametri\n",
    "unet_param_count = sum(p.numel() for p in original_unet.parameters() if p.requires_grad)\n",
    "accelerator.print(f\"UNet initialized with {unet_param_count / 1e6:.2f}M parameters.\")\n",
    "#Inizializza EMA model\n",
    "ema_unet = EMAModel(original_unet.parameters(), model_cls=UNet2DConditionModel, model_config=original_unet.config)\n",
    "ema_unet.to(accelerator.device) #E lo butto sulla GPU\n",
    "#Butto anche gli altri 2 coglioni sulla gpu, ma non allenandoli metto require_grad a false per risparmiare memoria\n",
    "vae.to(accelerator.device).eval().requires_grad_(False)\n",
    "text_encoder.to(accelerator.device).eval().requires_grad_(False)\n",
    "\n",
    "#Embedding vuoto per CFG invece di calcolarlo ogni volta come uno stupido\n",
    "with torch.no_grad():\n",
    "    uncond_tokens = tokenizer([\"\"], padding=\"max_length\", max_length=tokenizer.model_max_length, return_tensors=\"pt\").input_ids\n",
    "    uncond_embeddings = text_encoder(uncond_tokens.to(accelerator.device))[0]\n",
    "\n",
    "#Provo ad utilizzare XFormers per risparmiare memoria\n",
    "if xformers.version:\n",
    "    try:\n",
    "        original_unet.enable_xformers_memory_efficient_attention()\n",
    "        accelerator.print(\"XFormers memory efficient attention enabled for UNet.\")\n",
    "    except Exception as e:\n",
    "        accelerator.print(f\"Could not enable xformers memory efficient attention: {e}\")\n",
    "else:\n",
    "    accelerator.print(\"XFormers not available or not installed. Skipping memory efficient attention.\") #Cacca\n",
    "#Qua prima compilavo, ora non compilo piu il modello\n",
    "compiled_unet_object = None #Se non compilo lascio a none\n",
    "accelerator.print(\"Skipping UNet compilation to avoid potential CUDAGraph issues.\") # Updated message\n",
    "unet_to_prepare = original_unet # Directly use the original, uncompiled UNet\n",
    "\n",
    "# --- Noise Scheduler ---\n",
    "noise_scheduler = DDPMScheduler(\n",
    "    num_train_timesteps=num_train_timesteps_scheduler,\n",
    "    beta_schedule=\"scaled_linear\",\n",
    "    prediction_type=\"v_prediction\" # Switched to v-prediction\n",
    ")\n",
    "\n",
    "# --- Optimizer ---\n",
    "optimizer = AdamW(\n",
    "    unet_to_prepare.parameters(), # Optimize parameters of the (potentially compiled) UNet\n",
    "    lr=learning_rate, betas=(adam_beta1, adam_beta2),\n",
    "    weight_decay=adam_weight_decay, eps=adam_epsilon,\n",
    "    # Fused AdamW is often handled automatically by PyTorch/Accelerate or can be enabled if available\n",
    "    # fused=True if accelerator.device.type == 'cuda' and mixed_precision in [\"fp16\", \"bf16\"] else False # Be cautious with fused and compilation\n",
    ")\n",
    "\n",
    "# --- Learning Rate Scheduler ---\n",
    "def lr_lambda_cosine(current_step: int):\n",
    "    if num_warmup_steps > 0 and current_step < num_warmup_steps:\n",
    "        return float(current_step) / float(max(1, num_warmup_steps))\n",
    "    denominator = float(max(1, max_train_steps - num_warmup_steps))\n",
    "    progress = float(current_step - num_warmup_steps) / denominator\n",
    "    return max(0.0, 0.5 * (1.0 + math.cos(math.pi * progress)))\n",
    "\n",
    "lr_scheduler = LambdaLR(optimizer, lr_lambda_cosine)\n",
    "\n",
    "\n",
    "# --- Sistemo img ---\n",
    "image_transforms = transforms.Compose([\n",
    "    transforms.Resize((image_resolution, image_resolution), interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5]), # Normalize to [-1, 1]\n",
    "])\n",
    "#Preprocessing\n",
    "def preprocess_function(examples, tokenizer_obj, image_transforms_fn, accelerator_ref):\n",
    "    prompts, pil_images = examples[\"txt\"], examples[\"jpg\"]\n",
    "    processed_images, valid_prompts = [], []\n",
    "    for idx, (img_pil, prompt_text) in enumerate(zip(pil_images, prompts)):\n",
    "        try:\n",
    "            if img_pil is None:\n",
    "                if accelerator_ref.is_main_process: accelerator_ref.print(f\"Warning: Found None image for prompt: {prompt_text}. Skipping.\")\n",
    "                continue\n",
    "            img = img_pil.convert(\"RGB\")\n",
    "            img = ImageOps.exif_transpose(img) # Handle EXIF orientation\n",
    "            processed_images.append(image_transforms_fn(img))\n",
    "            valid_prompts.append(prompt_text)\n",
    "        except Exception as e:\n",
    "            if accelerator_ref.is_main_process: # Log only on main process to avoid spam\n",
    "                accelerator_ref.print(f\"Warning: Skipping an image/prompt due to error: {e}. Prompt: '{prompt_text}'. Image index in batch: {idx}\")\n",
    "            continue\n",
    "    \n",
    "    if not processed_images: # All images in this chunk were bad\n",
    "        return None\n",
    "        \n",
    "    text_inputs = tokenizer_obj(valid_prompts, padding=\"max_length\", max_length=tokenizer_obj.model_max_length, truncation=True, return_tensors=\"pt\")\n",
    "    return {\"pixel_values\": torch.stack(processed_images), \"input_ids\": text_inputs.input_ids}\n",
    "\n",
    "#Streaming DS\n",
    "class StreamingImageTextDataset(IterableDataset):\n",
    "    def __init__(self, dataset_id, split, transform_fn, tokenizer_obj, image_transforms_fn, processing_chunk_size, accelerator_ref):\n",
    "        self.dataset = load_dataset(dataset_id, split=split, streaming=True)\n",
    "        self.transform_fn = transform_fn\n",
    "        self.tokenizer_obj = tokenizer_obj\n",
    "        self.image_transforms_fn = image_transforms_fn\n",
    "        self.processing_chunk_size = processing_chunk_size\n",
    "        self.accelerator_ref = accelerator_ref # For logging inside preprocess\n",
    "\n",
    "    def __iter__(self):\n",
    "        buffer = []\n",
    "        for example in self.dataset:\n",
    "            # Ensure 'jpg' and 'txt' keys exist and are not None\n",
    "            if example.get(\"jpg\") is not None and example.get(\"txt\") is not None:\n",
    "                buffer.append({\"jpg\": example[\"jpg\"], \"txt\": example[\"txt\"]})\n",
    "            else:\n",
    "                if self.accelerator_ref.is_main_process:\n",
    "                    self.accelerator_ref.print(f\"Warning: Skipping example due to missing 'jpg' or 'txt' field: {example.get('txt', 'N/A')}\")\n",
    "                continue\n",
    "\n",
    "            if len(buffer) == self.processing_chunk_size:\n",
    "                processed_batch = self.transform_fn(\n",
    "                    {\"jpg\": [i[\"jpg\"] for i in buffer], \"txt\": [i[\"txt\"] for i in buffer]},\n",
    "                    self.tokenizer_obj, self.image_transforms_fn, self.accelerator_ref\n",
    "                )\n",
    "                if processed_batch:\n",
    "                    for i in range(processed_batch[\"pixel_values\"].size(0)):\n",
    "                        yield {\"pixel_values\": processed_batch[\"pixel_values\"][i], \"input_ids\": processed_batch[\"input_ids\"][i]}\n",
    "                buffer = []\n",
    "        \n",
    "        # Process any remaining items in the buffer\n",
    "        if buffer:\n",
    "            processed_batch = self.transform_fn(\n",
    "                {\"jpg\": [i[\"jpg\"] for i in buffer], \"txt\": [i[\"txt\"] for i in buffer]},\n",
    "                self.tokenizer_obj, self.image_transforms_fn, self.accelerator_ref\n",
    "            )\n",
    "            if processed_batch:\n",
    "                for i in range(processed_batch[\"pixel_values\"].size(0)):\n",
    "                    yield {\"pixel_values\": processed_batch[\"pixel_values\"][i], \"input_ids\": processed_batch[\"input_ids\"][i]}\n",
    "\n",
    "accelerator.print(\"Setting up dataset stream...\")\n",
    "#Num di sample passati insieme alla funzione di preprocessing\n",
    "#Evito di chiamare la stessa funzione piu volte cosi\n",
    "transform_processing_chunk_size = 1\n",
    "#Numero di thread che svolgono questo lavoro di merda\n",
    "num_dataloader_workers = 8\n",
    "\n",
    "#Bon, metto tutto insieme\n",
    "train_dataset = StreamingImageTextDataset(\n",
    "    dataset_id=dataset_id, split=\"train\", \n",
    "    transform_fn=preprocess_function, \n",
    "    tokenizer_obj=tokenizer, \n",
    "    image_transforms_fn=image_transforms,\n",
    "    processing_chunk_size=transform_processing_chunk_size,\n",
    "    accelerator_ref=accelerator\n",
    ")\n",
    "#Sistemo tutto\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, batch_size=train_batch_size, \n",
    "    num_workers=num_dataloader_workers, \n",
    "    pin_memory=True if num_dataloader_workers > 0 else False, # Pin memory if using workers\n",
    "    persistent_workers=True if num_dataloader_workers > 0 else False\n",
    ")\n",
    "#Daje, ci si prepara\n",
    "unet, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
    "    unet_to_prepare, optimizer, train_dataloader, lr_scheduler\n",
    ")\n",
    "\n",
    "# --- Resume from Checkpoint perche non so se la macchina a meta schioppa ---\n",
    "global_step, start_epoch, resume_wandb_id = 0, 0, None\n",
    "resume_wandb_id = None\n",
    "try:\n",
    "    global_step, start_epoch, resume_wandb_id = load_latest_checkpoint_custom(\n",
    "        accelerator, accelerator.unwrap_model(unet), ema_unet, optimizer, lr_scheduler, checkpoint_dir_base\n",
    "    )\n",
    "    if global_step > 0:\n",
    "        accelerator.print(f\"Resumed training from step {global_step}, epoch {start_epoch}.\")\n",
    "    else:\n",
    "        accelerator.print(\"Starting training from scratch or no compatible checkpoint found.\")\n",
    "except Exception as e:\n",
    "    accelerator.print(f\"Could not load checkpoint. Starting from scratch. Error: {e}\")\n",
    "    global_step, start_epoch, resume_wandb_id = 0, 0, None\n",
    "ema_unet.to(accelerator.device)\n",
    "\n",
    "# --- WandB Initialization ---\n",
    "if use_wandb and accelerator.is_main_process:\n",
    "    wandb_config = {\n",
    "        \"learning_rate\": learning_rate, \"train_batch_size\": train_batch_size,\n",
    "        \"gradient_accumulation_steps\": gradient_accumulation_steps,\n",
    "        \"effective_batch_size\": train_batch_size * accelerator.num_processes * gradient_accumulation_steps,\n",
    "        \"image_resolution\": image_resolution, \"unet_params_M\": unet_param_count / 1e6,\n",
    "        \"vae_model_id\": vae_model_id, \"text_encoder_model_id\": text_encoder_model_id,\n",
    "        \"dataset_id\": dataset_id, \"max_train_steps\": max_train_steps,\n",
    "        \"num_warmup_steps\": num_warmup_steps, \"mixed_precision\": mixed_precision,\n",
    "        \"torch_compile_mode\": \"reduce-overhead\" if compiled_unet_object is not None else \"None\",\n",
    "        \"v_prediction\": True, \"cfg_drop_probability\": cfg_drop_probability, \"seed\": seed,\n",
    "    }\n",
    "    #Resume se ce un checkpoint\n",
    "    run_id_to_resume = resume_wandb_id if global_step > 0 and resume_wandb_id else None\n",
    "    \n",
    "    accelerator.init_trackers(\n",
    "        project_name=wandb_project_name,\n",
    "        config=wandb_config,\n",
    "        init_kwargs={\"wandb\": {\"id\": run_id_to_resume, \"resume\": \"allow\"}}\n",
    "    )\n",
    "    if run_id_to_resume: accelerator.print(f\"Resuming WandB run with ID: {run_id_to_resume}\")\n",
    "    elif global_step > 0: accelerator.print(\"Warning: Resuming training but no wandb_run_id found in checkpoint for WandB.\")\n",
    "\n",
    "\n",
    "# --- Training Loop ---\n",
    "accelerator.print(f\"Starting training. Target steps: {max_train_steps}. Current step: {global_step}. Warmup steps: {num_warmup_steps}.\")\n",
    "progress_bar = tqdm(initial=global_step, total=max_train_steps, desc=\"Training Steps\", disable=not accelerator.is_main_process)\n",
    "current_epoch_for_tracking = start_epoch # For saving in checkpoint\n",
    "\n",
    "unet_for_sampling_config = accelerator.unwrap_model(unet).config\n",
    "\n",
    "# Main training loop\n",
    "while global_step < max_train_steps:\n",
    "    unet.train()\n",
    "    for batch_idx, batch in enumerate(train_dataloader):\n",
    "        if global_step >= max_train_steps:\n",
    "            break\n",
    "\n",
    "        with accelerator.accumulate(unet): #Gradient acc\n",
    "            pixel_values = batch[\"pixel_values\"]\n",
    "            input_ids = batch[\"input_ids\"]\n",
    "            bsz = pixel_values.shape[0]\n",
    "\n",
    "            with torch.no_grad():\n",
    "                latents = vae.encode(pixel_values).latent_dist.sample() * vae.config.scaling_factor\n",
    "                encoder_hidden_states = text_encoder(input_ids)[0]\n",
    "\n",
    "            # Classifier-Free Guidance\n",
    "            mask = torch.rand(bsz, device=accelerator.device) < cfg_drop_probability\n",
    "            if mask.any():\n",
    "                 encoder_hidden_states[mask] = uncond_embeddings.expand(mask.sum(), -1, -1)\n",
    "\n",
    "            noise = torch.randn_like(latents)\n",
    "            timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (bsz,), device=latents.device).long()\n",
    "            noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
    "            \n",
    "            #V-prediction invece di image o noise prediction (via di mezzo, dovrebbe funzionare anche se male)\n",
    "            target_velocity = noise_scheduler.get_velocity(latents, noise, timesteps)\n",
    "\n",
    "            #Mixed precision\n",
    "            model_pred = unet(noisy_latents, timesteps, encoder_hidden_states).sample\n",
    "            loss = F.mse_loss(model_pred.float(), target_velocity.float(), reduction=\"mean\")\n",
    "\n",
    "            accelerator.backward(loss)\n",
    "            if accelerator.sync_gradients: # Only clip when gradients are synced (after accumulation)\n",
    "                accelerator.clip_grad_norm_(unet.parameters(), max_grad_norm)\n",
    "            \n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        #Usa param della Unet originale\n",
    "        if accelerator.sync_gradients: #Update EMA unet\n",
    "            ema_unet.step(accelerator.unwrap_model(unet).parameters())\n",
    "\n",
    "        if accelerator.sync_gradients: #Progress barrrrrrr\n",
    "            progress_bar.update(1)\n",
    "            global_step += 1\n",
    "            \n",
    "            if global_step % 100 == 0:\n",
    "                current_lr = optimizer.param_groups[0]['lr']\n",
    "                log_data = {\"train_loss\": loss.item(), \"learning_rate\": current_lr, \"global_step\": global_step}\n",
    "                accelerator.log(log_data, step=global_step)\n",
    "                progress_bar.set_postfix({\"loss\": f\"{loss.item():.4f}\", \"lr\": f\"{current_lr:.2e}\"})\n",
    "\n",
    "                #Salva\n",
    "            if global_step > 0 and global_step % save_every_steps == 0 and save_every_steps > 0:\n",
    "                if accelerator.is_main_process:\n",
    "                    save_checkpoint_custom(\n",
    "                        accelerator,\n",
    "                        accelerator.unwrap_model(unet), #original model\n",
    "                        ema_unet, #EMA model instance\n",
    "                        optimizer, #prepared optimizer\n",
    "                        lr_scheduler, #prepared scheduler\n",
    "                        global_step, #step a cui siamo arrivati\n",
    "                        current_epoch_for_tracking, #book-keeping, non tanto necessario\n",
    "                        checkpoint_dir_base,\n",
    "                        max_keep=max_keep_checkpoints\n",
    "                    )\n",
    "                    \n",
    "                    #Genera qualche img per i mie occhietti curiosi\n",
    "                    unet_ema_sample_model = UNet2DConditionModel.from_config(unet_for_sampling_config).to(accelerator.device)\n",
    "                    ema_unet.copy_to(unet_ema_sample_model.parameters()) # Copy EMA params to this new model\n",
    "                    \n",
    "                    generate_samples(\n",
    "                        unet_ema_sample_model, vae, text_encoder, tokenizer,\n",
    "                        noise_scheduler, sampling_prompts, output_dir_base,\n",
    "                        global_step, accelerator.device, accelerator\n",
    "                    )\n",
    "                    del unet_ema_sample_model #Libera mem\n",
    "                    if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
    "            \n",
    "            if global_step >= max_train_steps:\n",
    "                break\n",
    "    \n",
    "    #Esce quando gli step sono stati fatti\n",
    "    if global_step >= max_train_steps:\n",
    "        break\n",
    "\n",
    "\n",
    "progress_bar.close()\n",
    "accelerator.print(\"Training finished.\")\n",
    "\n",
    "# --- Salva modello finale ---\n",
    "if accelerator.is_main_process:\n",
    "    accelerator.print(f\"Saving final model and components to {final_model_dir}\")\n",
    "    os.makedirs(final_model_dir, exist_ok=True)\n",
    "    unwrapped_unet = accelerator.unwrap_model(unet)\n",
    "    unwrapped_unet.save_pretrained(os.path.join(final_model_dir, \"unet\"))\n",
    "    final_ema_unet_model = UNet2DConditionModel.from_config(unwrapped_unet.config).to(accelerator.device)\n",
    "    ema_unet.copy_to(final_ema_unet_model.parameters())\n",
    "    final_ema_unet_model.save_pretrained(os.path.join(final_model_dir, \"ema_unet\"))\n",
    "    del final_ema_unet_model\n",
    "    vae.save_pretrained(os.path.join(final_model_dir, \"vae\"))\n",
    "    text_encoder.save_pretrained(os.path.join(final_model_dir, \"text_encoder\"))\n",
    "    tokenizer.save_pretrained(os.path.join(final_model_dir, \"tokenizer\"))\n",
    "    noise_scheduler.save_config(os.path.join(final_model_dir, \"scheduler\")) # Saves config.json\n",
    "    accelerator.save_state(os.path.join(final_model_dir, \"accelerator_state\"))\n",
    "    torch.save(ema_unet.state_dict(), os.path.join(final_model_dir, \"ema_unet_final_state.pth\"))\n",
    "    final_custom_state = {\n",
    "        'global_step': global_step, 'epoch': current_epoch_for_tracking,\n",
    "        'wandb_run_id': wandb.run.id if use_wandb and wandb.run else None\n",
    "    }\n",
    "    torch.save(final_custom_state, os.path.join(final_model_dir, \"final_custom_training_state.pt\"))\n",
    "    if use_wandb and wandb.run:\n",
    "        try:\n",
    "            final_model_artifact = wandb.Artifact(\n",
    "                name=f\"{wandb_project_name.lower().replace(' ', '_')}-final_model\",\n",
    "                type=\"model\",\n",
    "                description=f\"Final trained diffusion model components at step {global_step}.\",\n",
    "                metadata=wandb_config\n",
    "            )\n",
    "            final_model_artifact.add_dir(final_model_dir)\n",
    "            wandb.log_artifact(final_model_artifact)\n",
    "            accelerator.print(\"Final model saved as WandB artifact.\")\n",
    "        except Exception as e:\n",
    "            accelerator.print(f\"Failed to save model as WandB artifact: {e}\")\n",
    "\n",
    "\n",
    "accelerator.end_training()\n",
    "accelerator.print(\"All components saved. Training script complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed22a58a-797a-4338-9439-7fb49ca66d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mega test se la pipeline e il problema...\n",
    "#Non e il problema...\n",
    "\"\"\"\n",
    "import os\n",
    "import io\n",
    "import glob\n",
    "import shutil\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import math\n",
    "from torch.utils.data import DataLoader, IterableDataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image, ImageOps\n",
    "from tqdm.auto import tqdm\n",
    "import wandb\n",
    "import xformers # Added for XFORMERS\n",
    "\n",
    "# Hugging Face Libraries\n",
    "from diffusers import AutoencoderKL, UNet2DConditionModel, DDPMScheduler\n",
    "from diffusers.training_utils import EMAModel\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from datasets import load_dataset\n",
    "from accelerate import Accelerator, DistributedDataParallelKwargs\n",
    "from accelerate.utils import ProjectConfiguration, set_seed, LoggerType\n",
    "\n",
    "# --- Configuration (Hardcoded for simplicity) ---\n",
    "use_dummy_data = True  # <<< SET THIS TO True TO USE DUMMY DATA FOR BOTTLENECK TESTING\n",
    "# If True, the script will bypass dataset loading and use randomly generated tensors.\n",
    "\n",
    "vae_model_id = \"stabilityai/sd-vae-ft-ema\"\n",
    "text_encoder_model_id = \"openai/clip-vit-large-patch14\"\n",
    "dataset_id = \"BLIP3o/BLIP3o-Pretrain-Short-Caption\" # Used only if use_dummy_data is False\n",
    "\n",
    "# Training Hyperparameters\n",
    "image_resolution = 512\n",
    "latent_resolution = image_resolution // 8\n",
    "train_batch_size = 16\n",
    "gradient_accumulation_steps = 8\n",
    "learning_rate = 1e-4\n",
    "adam_beta1 = 0.9\n",
    "adam_beta2 = 0.999\n",
    "adam_weight_decay = 1e-2\n",
    "adam_epsilon = 1e-08\n",
    "max_grad_norm = 1.0\n",
    "num_train_timesteps_scheduler = 1000\n",
    "mixed_precision = \"bf16\"\n",
    "seed = 42\n",
    "cfg_drop_probability = 0.1\n",
    "max_train_steps = 30_000\n",
    "save_every_steps = max_train_steps // 100\n",
    "num_warmup_steps_ratio = 0.05\n",
    "num_warmup_steps = int(num_warmup_steps_ratio * max_train_steps)\n",
    "# --- Storage Path ---\n",
    "persistent_storage_mount_path = \"/workspace\"\n",
    "project_data_folder_name = \"PozzyDiffusion\"\n",
    "output_dir_base = os.path.join(persistent_storage_mount_path, project_data_folder_name)\n",
    "logging_dir = os.path.join(output_dir_base, \"logs\")\n",
    "checkpoint_dir_base = os.path.join(output_dir_base, \"checkpoints\")\n",
    "final_model_dir = os.path.join(output_dir_base, \"final_model\")\n",
    "max_keep_checkpoints = 2\n",
    "# WandB\n",
    "wandb_project_name = \"PozzyDiffusion_A40\"\n",
    "use_wandb = False # Set to False if you want to run dummy data test without WandB logging\n",
    "# Hardcoded prompts per sampling\n",
    "sampling_prompts = [\n",
    "    \"a photo of an astronaut riding a horse on mars\",\n",
    "    \"a fantasy landscape with a castle and a dragon\",\n",
    "    \"a cyberpunk city at night with neon lights\",\n",
    "    \"a serene Japanese garden with cherry blossoms\",\n",
    "    \"a whimsical illustration of a cat playing a piano\"\n",
    "]\n",
    "num_inference_steps_sampling = 50\n",
    "\n",
    "# --- Accelerator ---\n",
    "project_config = ProjectConfiguration(project_dir=output_dir_base, logging_dir=logging_dir)\n",
    "accelerator = Accelerator(\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    mixed_precision=mixed_precision,\n",
    "    log_with=\"wandb\" if use_wandb and not use_dummy_data else None, # Optionally disable wandb for dummy runs\n",
    "    project_config=project_config,\n",
    ")\n",
    "if seed is not None:\n",
    "    set_seed(seed)\n",
    "if accelerator.is_main_process:\n",
    "    os.makedirs(output_dir_base, exist_ok=True)\n",
    "    os.makedirs(checkpoint_dir_base, exist_ok=True)\n",
    "    os.makedirs(final_model_dir, exist_ok=True)\n",
    "\n",
    "# --- Helper Functions ---\n",
    "def save_checkpoint_custom(accelerator, unet_original, ema_unet, optimizer, lr_scheduler, global_step, epoch, checkpoint_dir_base, filename_prefix=\"ckpt\", max_keep=2):\n",
    "    if accelerator.is_main_process:\n",
    "        os.makedirs(checkpoint_dir_base, exist_ok=True)\n",
    "        checkpoint_name = f\"{filename_prefix}_step{global_step}\"\n",
    "        save_path = os.path.join(checkpoint_dir_base, checkpoint_name)\n",
    "        accelerator.save_state(save_path)\n",
    "        ema_save_path = os.path.join(save_path, \"ema_unet.pth\")\n",
    "        torch.save(ema_unet.state_dict(), ema_save_path)\n",
    "        custom_state = {\n",
    "            'global_step': global_step, 'epoch': epoch,\n",
    "            'wandb_run_id': wandb.run.id if use_wandb and wandb.run and not use_dummy_data else None\n",
    "        }\n",
    "        torch.save(custom_state, os.path.join(save_path, \"custom_training_state.pt\"))\n",
    "        accelerator.print(f\"Saved checkpoint: {save_path}\")\n",
    "        checkpoints = sorted(\n",
    "            glob.glob(os.path.join(checkpoint_dir_base, f\"{filename_prefix}_step*\")),\n",
    "            key=lambda x: int(os.path.basename(x).split('step')[-1])\n",
    "        )\n",
    "        if len(checkpoints) > max_keep:\n",
    "            for old_ckpt_path in checkpoints[:-max_keep]:\n",
    "                accelerator.print(f\"Removing old checkpoint: {old_ckpt_path}\")\n",
    "                shutil.rmtree(old_ckpt_path)\n",
    "\n",
    "def load_latest_checkpoint_custom(accelerator, unet_original, ema_unet, optimizer, lr_scheduler, checkpoint_dir_base, filename_prefix=\"ckpt\"):\n",
    "    checkpoints = sorted(\n",
    "        glob.glob(os.path.join(checkpoint_dir_base, f\"{filename_prefix}_step*\")),\n",
    "        key=lambda x: int(os.path.basename(x).split('step')[-1]),\n",
    "        reverse=True\n",
    "    )\n",
    "    if checkpoints:\n",
    "        latest_checkpoint_path = checkpoints[0]\n",
    "        accelerator.print(f\"Found checkpoint: {latest_checkpoint_path}\")\n",
    "        try:\n",
    "            accelerator.load_state(latest_checkpoint_path)\n",
    "            ema_path = os.path.join(latest_checkpoint_path, \"ema_unet.pth\")\n",
    "            if os.path.exists(ema_path):\n",
    "                ema_unet.load_state_dict(torch.load(ema_path, map_location=\"cpu\"))\n",
    "                accelerator.print(\"EMA UNet state loaded.\")\n",
    "            custom_state_path = os.path.join(latest_checkpoint_path, \"custom_training_state.pt\")\n",
    "            if os.path.exists(custom_state_path):\n",
    "                custom_state = torch.load(custom_state_path, map_location=\"cpu\")\n",
    "                return custom_state.get('global_step', 0), custom_state.get('epoch', 0), custom_state.get('wandb_run_id')\n",
    "            else:\n",
    "                parsed_step = int(os.path.basename(latest_checkpoint_path).split('step')[-1])\n",
    "                return parsed_step, 0, None\n",
    "        except Exception as e:\n",
    "            accelerator.print(f\"Error loading checkpoint {latest_checkpoint_path}: {e}. Starting fresh.\")\n",
    "            return 0, 0, None\n",
    "    return 0, 0, None\n",
    "\n",
    "def generate_samples(unet_model_for_sampling, vae, text_encoder, tokenizer_obj, noise_scheduler_obj, prompts, output_dir, global_step, device, accelerator_ref):\n",
    "    # This function might be less meaningful with dummy data, but can still run.\n",
    "    # Consider skipping if use_dummy_data is True and samples are not needed for the test.\n",
    "    if use_dummy_data and accelerator_ref.is_main_process:\n",
    "        accelerator_ref.print(\"Skipping sample generation during dummy data run.\")\n",
    "        return\n",
    "\n",
    "    unet_model_for_sampling.eval()\n",
    "    vae.eval()\n",
    "    text_encoder.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, prompt in enumerate(prompts):\n",
    "            text_inputs = tokenizer_obj(prompt, padding=\"max_length\", max_length=tokenizer_obj.model_max_length, truncation=True, return_tensors=\"pt\")\n",
    "            text_embeddings = text_encoder(text_inputs.input_ids.to(device))[0]\n",
    "            latents_shape = (1, unet_model_for_sampling.config.in_channels, latent_resolution, latent_resolution)\n",
    "            latents = torch.randn(latents_shape, device=device, generator=torch.manual_seed(seed + i) if seed is not None else None)\n",
    "            noise_scheduler_obj.set_timesteps(num_inference_steps_sampling)\n",
    "            for t in tqdm(noise_scheduler_obj.timesteps, desc=f\"Sampling for prompt {i+1}\", disable=not accelerator_ref.is_main_process):\n",
    "                latent_model_input = noise_scheduler_obj.scale_model_input(latents, t)\n",
    "                noise_pred = unet_model_for_sampling(latent_model_input, t, encoder_hidden_states=text_embeddings).sample\n",
    "                latents = noise_scheduler_obj.step(noise_pred, t, latents).prev_sample\n",
    "            latents = 1 / vae.config.scaling_factor * latents\n",
    "            image = vae.decode(latents).sample\n",
    "            image = (image / 2 + 0.5).clamp(0, 1)\n",
    "            image = image.cpu().permute(0, 2, 3, 1).numpy()[0]\n",
    "            image = Image.fromarray((image * 255).round().astype(\"uint8\"))\n",
    "            if accelerator_ref.is_main_process:\n",
    "                sample_output_dir = os.path.join(output_dir, \"samples\", f\"step_{global_step}\")\n",
    "                os.makedirs(sample_output_dir, exist_ok=True)\n",
    "                img_path = os.path.join(sample_output_dir, f\"prompt_{i+1}_seed{seed+i if seed is not None else 'rand'}.png\")\n",
    "                image.save(img_path)\n",
    "                accelerator_ref.print(f\"Generated sample for prompt '{prompt}' saved to {img_path}\")\n",
    "    unet_model_for_sampling.train()\n",
    "\n",
    "\n",
    "# --- Dummy Data Iterable Dataset ---\n",
    "class DummyIterableDataset(IterableDataset):\n",
    "    def __init__(self, image_res, tokenizer_max_len, pixel_dtype=torch.float32, input_id_dtype=torch.long):\n",
    "        super().__init__()\n",
    "        self.image_resolution = image_res\n",
    "        self.tokenizer_max_length = tokenizer_max_len\n",
    "        self.pixel_dtype = pixel_dtype\n",
    "        self.input_id_dtype = input_id_dtype\n",
    "\n",
    "    def __iter__(self):\n",
    "        while True:\n",
    "            dummy_pixel_values = torch.randn(\n",
    "                3, self.image_resolution, self.image_resolution, dtype=self.pixel_dtype\n",
    "            )\n",
    "            dummy_input_ids = torch.ones(\n",
    "                self.tokenizer_max_length, dtype=self.input_id_dtype\n",
    "            )\n",
    "            # You could use torch.randint for more varied input_ids if needed:\n",
    "            # dummy_input_ids = torch.randint(0, 30000, (self.tokenizer_max_length,), dtype=self.input_id_dtype) # Assumes vocab_size > 30000\n",
    "            yield {\"pixel_values\": dummy_pixel_values, \"input_ids\": dummy_input_ids}\n",
    "\n",
    "# --- Inizializza i modelli ---\n",
    "accelerator.print(\"Loading VAE...\")\n",
    "vae = AutoencoderKL.from_pretrained(vae_model_id)\n",
    "accelerator.print(\"Loading Text Encoder & Tokenizer...\")\n",
    "tokenizer = CLIPTokenizer.from_pretrained(text_encoder_model_id)\n",
    "text_encoder = CLIPTextModel.from_pretrained(text_encoder_model_id)\n",
    "accelerator.print(\"Initializing UNet...\")\n",
    "original_unet = UNet2DConditionModel(\n",
    "    sample_size=latent_resolution, in_channels=vae.config.latent_channels, out_channels=vae.config.latent_channels,\n",
    "    layers_per_block=2, block_out_channels=(256, 512, 768, 1024),\n",
    "    down_block_types=(\"CrossAttnDownBlock2D\", \"CrossAttnDownBlock2D\", \"CrossAttnDownBlock2D\", \"DownBlock2D\"),\n",
    "    up_block_types=(\"UpBlock2D\", \"CrossAttnUpBlock2D\", \"CrossAttnUpBlock2D\", \"CrossAttnUpBlock2D\"),\n",
    "    cross_attention_dim=text_encoder.config.hidden_size,\n",
    ")\n",
    "unet_param_count = sum(p.numel() for p in original_unet.parameters() if p.requires_grad)\n",
    "accelerator.print(f\"UNet initialized with {unet_param_count / 1e6:.2f}M parameters.\")\n",
    "ema_unet = EMAModel(original_unet.parameters(), model_cls=UNet2DConditionModel, model_config=original_unet.config)\n",
    "ema_unet.to(accelerator.device)\n",
    "vae.to(accelerator.device).eval().requires_grad_(False)\n",
    "text_encoder.to(accelerator.device).eval().requires_grad_(False)\n",
    "\n",
    "with torch.no_grad():\n",
    "    uncond_tokens = tokenizer([\"\"], padding=\"max_length\", max_length=tokenizer.model_max_length, return_tensors=\"pt\").input_ids\n",
    "    uncond_embeddings = text_encoder(uncond_tokens.to(accelerator.device))[0]\n",
    "\n",
    "if xformers.version:\n",
    "    try:\n",
    "        original_unet.enable_xformers_memory_efficient_attention()\n",
    "        accelerator.print(\"XFormers memory efficient attention enabled for UNet.\")\n",
    "    except Exception as e:\n",
    "        accelerator.print(f\"Could not enable xformers memory efficient attention: {e}\")\n",
    "else:\n",
    "    accelerator.print(\"XFormers not available or not installed. Skipping memory efficient attention.\")\n",
    "compiled_unet_object = None\n",
    "accelerator.print(\"Skipping UNet compilation to avoid potential CUDAGraph issues.\")\n",
    "unet_to_prepare = original_unet\n",
    "\n",
    "# --- Noise Scheduler ---\n",
    "noise_scheduler = DDPMScheduler(\n",
    "    num_train_timesteps=num_train_timesteps_scheduler,\n",
    "    beta_schedule=\"scaled_linear\",\n",
    "    prediction_type=\"v_prediction\"\n",
    ")\n",
    "\n",
    "# --- Optimizer ---\n",
    "optimizer = AdamW(\n",
    "    unet_to_prepare.parameters(), lr=learning_rate, betas=(adam_beta1, adam_beta2),\n",
    "    weight_decay=adam_weight_decay, eps=adam_epsilon,\n",
    ")\n",
    "\n",
    "# --- Learning Rate Scheduler ---\n",
    "def lr_lambda_cosine(current_step: int):\n",
    "    if num_warmup_steps > 0 and current_step < num_warmup_steps:\n",
    "        return float(current_step) / float(max(1, num_warmup_steps))\n",
    "    denominator = float(max(1, max_train_steps - num_warmup_steps))\n",
    "    progress = float(current_step - num_warmup_steps) / denominator\n",
    "    return max(0.0, 0.5 * (1.0 + math.cos(math.pi * progress)))\n",
    "lr_scheduler = LambdaLR(optimizer, lr_lambda_cosine)\n",
    "\n",
    "\n",
    "# --- Sistemo img (Real Data Path) ---\n",
    "image_transforms = transforms.Compose([\n",
    "    transforms.Resize((image_resolution, image_resolution), interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "    transforms.ToTensor(), # Converts to [0,1] range, float32\n",
    "    transforms.Normalize([0.5], [0.5]), # Normalize to [-1, 1]\n",
    "])\n",
    "def preprocess_function(examples, tokenizer_obj, image_transforms_fn, accelerator_ref):\n",
    "    prompts, pil_images = examples[\"txt\"], examples[\"jpg\"]\n",
    "    processed_images, valid_prompts = [], []\n",
    "    for idx, (img_pil, prompt_text) in enumerate(zip(pil_images, prompts)):\n",
    "        try:\n",
    "            if img_pil is None:\n",
    "                if accelerator_ref.is_main_process: accelerator_ref.print(f\"Warning: Found None image for prompt: {prompt_text}. Skipping.\")\n",
    "                continue\n",
    "            img = img_pil.convert(\"RGB\")\n",
    "            img = ImageOps.exif_transpose(img)\n",
    "            processed_images.append(image_transforms_fn(img))\n",
    "            valid_prompts.append(prompt_text)\n",
    "        except Exception as e:\n",
    "            if accelerator_ref.is_main_process:\n",
    "                accelerator_ref.print(f\"Warning: Skipping an image/prompt due to error: {e}. Prompt: '{prompt_text}'. Image index in batch: {idx}\")\n",
    "            continue\n",
    "    if not processed_images:\n",
    "        return None\n",
    "    text_inputs = tokenizer_obj(valid_prompts, padding=\"max_length\", max_length=tokenizer_obj.model_max_length, truncation=True, return_tensors=\"pt\")\n",
    "    return {\"pixel_values\": torch.stack(processed_images), \"input_ids\": text_inputs.input_ids}\n",
    "\n",
    "class StreamingImageTextDataset(IterableDataset):\n",
    "    def __init__(self, dataset_id_str, split, transform_fn, tokenizer_obj, image_transforms_fn, processing_chunk_size, accelerator_ref):\n",
    "        self.dataset = load_dataset(dataset_id_str, split=split, streaming=True)\n",
    "        self.transform_fn = transform_fn\n",
    "        self.tokenizer_obj = tokenizer_obj\n",
    "        self.image_transforms_fn = image_transforms_fn\n",
    "        self.processing_chunk_size = processing_chunk_size\n",
    "        self.accelerator_ref = accelerator_ref\n",
    "\n",
    "    def __iter__(self):\n",
    "        buffer = []\n",
    "        for example in self.dataset:\n",
    "            if example.get(\"jpg\") is not None and example.get(\"txt\") is not None:\n",
    "                buffer.append({\"jpg\": example[\"jpg\"], \"txt\": example[\"txt\"]})\n",
    "            else:\n",
    "                if self.accelerator_ref.is_main_process:\n",
    "                    self.accelerator_ref.print(f\"Warning: Skipping example due to missing 'jpg' or 'txt' field: {example.get('txt', 'N/A')}\")\n",
    "                continue\n",
    "            if len(buffer) == self.processing_chunk_size:\n",
    "                processed_batch = self.transform_fn(\n",
    "                    {\"jpg\": [i[\"jpg\"] for i in buffer], \"txt\": [i[\"txt\"] for i in buffer]},\n",
    "                    self.tokenizer_obj, self.image_transforms_fn, self.accelerator_ref\n",
    "                )\n",
    "                if processed_batch:\n",
    "                    for i in range(processed_batch[\"pixel_values\"].size(0)):\n",
    "                        yield {\"pixel_values\": processed_batch[\"pixel_values\"][i], \"input_ids\": processed_batch[\"input_ids\"][i]}\n",
    "                buffer = []\n",
    "        if buffer:\n",
    "            processed_batch = self.transform_fn(\n",
    "                {\"jpg\": [i[\"jpg\"] for i in buffer], \"txt\": [i[\"txt\"] for i in buffer]},\n",
    "                self.tokenizer_obj, self.image_transforms_fn, self.accelerator_ref\n",
    "            )\n",
    "            if processed_batch:\n",
    "                for i in range(processed_batch[\"pixel_values\"].size(0)):\n",
    "                    yield {\"pixel_values\": processed_batch[\"pixel_values\"][i], \"input_ids\": processed_batch[\"input_ids\"][i]}\n",
    "\n",
    "# --- DATALOADER SETUP (REAL OR DUMMY) ---\n",
    "transform_processing_chunk_size = 1 # Used only for real data\n",
    "num_dataloader_workers = 8\n",
    "\n",
    "if use_dummy_data:\n",
    "    accelerator.print(\"INFO: Using DUMMY DATA for training to test GPU throughput.\")\n",
    "    train_dataset = DummyIterableDataset(\n",
    "        image_res=image_resolution,\n",
    "        tokenizer_max_len=tokenizer.model_max_length, # Ensure tokenizer is loaded before this\n",
    "        pixel_dtype=torch.float32 # Matches output of image_transforms\n",
    "    )\n",
    "else:\n",
    "    accelerator.print(\"Setting up real dataset stream...\")\n",
    "    train_dataset = StreamingImageTextDataset(\n",
    "        dataset_id_str=dataset_id, split=\"train\",\n",
    "        transform_fn=preprocess_function,\n",
    "        tokenizer_obj=tokenizer,\n",
    "        image_transforms_fn=image_transforms,\n",
    "        processing_chunk_size=transform_processing_chunk_size,\n",
    "        accelerator_ref=accelerator\n",
    "    )\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, batch_size=train_batch_size,\n",
    "    num_workers=num_dataloader_workers,\n",
    "    pin_memory=True if num_dataloader_workers > 0 else False,\n",
    "    persistent_workers=True if num_dataloader_workers > 0 else False\n",
    ")\n",
    "\n",
    "unet, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
    "    unet_to_prepare, optimizer, train_dataloader, lr_scheduler\n",
    ")\n",
    "\n",
    "# --- Resume from Checkpoint ---\n",
    "global_step, start_epoch, resume_wandb_id = 0, 0, None\n",
    "if not use_dummy_data: # Checkpoints are usually for real data runs\n",
    "    try:\n",
    "        global_step, start_epoch, resume_wandb_id = load_latest_checkpoint_custom(\n",
    "            accelerator, accelerator.unwrap_model(unet), ema_unet, optimizer, lr_scheduler, checkpoint_dir_base\n",
    "        )\n",
    "        if global_step > 0:\n",
    "            accelerator.print(f\"Resumed training from step {global_step}, epoch {start_epoch}.\")\n",
    "        else:\n",
    "            accelerator.print(\"Starting training from scratch or no compatible checkpoint found.\")\n",
    "    except Exception as e:\n",
    "        accelerator.print(f\"Could not load checkpoint. Starting from scratch. Error: {e}\")\n",
    "        global_step, start_epoch, resume_wandb_id = 0, 0, None\n",
    "else:\n",
    "    accelerator.print(\"INFO: Dummy data run. Checkpoint loading skipped. Starting from step 0.\")\n",
    "\n",
    "\n",
    "# --- WandB Initialization ---\n",
    "if use_wandb and accelerator.is_main_process and not use_dummy_data: # Optionally skip wandb for dummy data\n",
    "    wandb_config = {\n",
    "        \"learning_rate\": learning_rate, \"train_batch_size\": train_batch_size,\n",
    "        \"gradient_accumulation_steps\": gradient_accumulation_steps,\n",
    "        \"effective_batch_size\": train_batch_size * accelerator.num_processes * gradient_accumulation_steps,\n",
    "        \"image_resolution\": image_resolution, \"unet_params_M\": unet_param_count / 1e6,\n",
    "        \"vae_model_id\": vae_model_id, \"text_encoder_model_id\": text_encoder_model_id,\n",
    "        \"dataset_id\": dataset_id, \"max_train_steps\": max_train_steps,\n",
    "        \"num_warmup_steps\": num_warmup_steps, \"mixed_precision\": mixed_precision,\n",
    "        \"torch_compile_mode\": \"reduce-overhead\" if compiled_unet_object is not None else \"None\",\n",
    "        \"v_prediction\": True, \"cfg_drop_probability\": cfg_drop_probability, \"seed\": seed,\n",
    "        \"using_dummy_data\": use_dummy_data\n",
    "    }\n",
    "    run_id_to_resume = resume_wandb_id if global_step > 0 and resume_wandb_id else None\n",
    "    accelerator.init_trackers(\n",
    "        project_name=wandb_project_name,\n",
    "        config=wandb_config,\n",
    "        init_kwargs={\"wandb\": {\"id\": run_id_to_resume, \"resume\": \"allow\", \"mode\": \"online\" if not use_dummy_data else \"disabled\"}}\n",
    "    )\n",
    "    if run_id_to_resume: accelerator.print(f\"Resuming WandB run with ID: {run_id_to_resume}\")\n",
    "    elif global_step > 0: accelerator.print(\"Warning: Resuming training but no wandb_run_id found in checkpoint for WandB.\")\n",
    "elif use_dummy_data and accelerator.is_main_process:\n",
    "    accelerator.print(\"INFO: Dummy data run. WandB logging is disabled or limited.\")\n",
    "\n",
    "\n",
    "# --- Training Loop ---\n",
    "accelerator.print(f\"Starting training. Target steps: {max_train_steps}. Current step: {global_step}. Warmup steps: {num_warmup_steps}.\")\n",
    "progress_bar = tqdm(initial=global_step, total=max_train_steps, desc=\"Training Steps\", disable=not accelerator.is_main_process)\n",
    "current_epoch_for_tracking = start_epoch\n",
    "unet_for_sampling_config = accelerator.unwrap_model(unet).config\n",
    "\n",
    "while global_step < max_train_steps:\n",
    "    unet.train()\n",
    "    for batch_idx, batch in enumerate(train_dataloader):\n",
    "        if global_step >= max_train_steps:\n",
    "            break\n",
    "\n",
    "        with accelerator.accumulate(unet):\n",
    "            # Data is expected to be on CPU from DataLoader, needs to be moved to GPU\n",
    "            # For VAE and TextEncoder, which are on accelerator.device\n",
    "            pixel_values = batch[\"pixel_values\"].to(accelerator.device)\n",
    "            input_ids = batch[\"input_ids\"].to(accelerator.device)\n",
    "            bsz = pixel_values.shape[0]\n",
    "\n",
    "            with torch.no_grad():\n",
    "                latents = vae.encode(pixel_values).latent_dist.sample() * vae.config.scaling_factor\n",
    "                encoder_hidden_states = text_encoder(input_ids)[0]\n",
    "\n",
    "            mask = torch.rand(bsz, device=accelerator.device) < cfg_drop_probability\n",
    "            if mask.any():\n",
    "                 encoder_hidden_states[mask] = uncond_embeddings.expand(mask.sum(), -1, -1)\n",
    "\n",
    "            noise = torch.randn_like(latents) # latents are on device\n",
    "            timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (bsz,), device=latents.device).long()\n",
    "            noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
    "            target_velocity = noise_scheduler.get_velocity(latents, noise, timesteps)\n",
    "\n",
    "            model_pred = unet(noisy_latents, timesteps, encoder_hidden_states).sample\n",
    "            loss = F.mse_loss(model_pred.float(), target_velocity.float(), reduction=\"mean\")\n",
    "\n",
    "            accelerator.backward(loss)\n",
    "            if accelerator.sync_gradients:\n",
    "                accelerator.clip_grad_norm_(unet.parameters(), max_grad_norm)\n",
    "            \n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        if accelerator.sync_gradients:\n",
    "            ema_unet.step(accelerator.unwrap_model(unet).parameters())\n",
    "            progress_bar.update(1)\n",
    "            global_step += 1\n",
    "            \n",
    "            # Logging (can be simplified or skipped for dummy data runs if desired)\n",
    "            if global_step % 100 == 0: # Log every 100 steps\n",
    "                current_lr = optimizer.param_groups[0]['lr']\n",
    "                log_data = {\"train_loss\": loss.item(), \"learning_rate\": current_lr, \"global_step\": global_step}\n",
    "                if use_wandb and not use_dummy_data : # Only log to wandb if it's enabled and not dummy data\n",
    "                    accelerator.log(log_data, step=global_step)\n",
    "                elif accelerator.is_main_process: # Print to console for dummy data or if wandb is off\n",
    "                    print(f\"Step: {global_step}, Loss: {loss.item():.4f}, LR: {current_lr:.2e}\")\n",
    "                progress_bar.set_postfix({\"loss\": f\"{loss.item():.4f}\", \"lr\": f\"{current_lr:.2e}\"})\n",
    "\n",
    "            # Save Checkpoint (usually skipped for dummy data runs)\n",
    "            if not use_dummy_data and global_step > 0 and global_step % save_every_steps == 0 and save_every_steps > 0:\n",
    "                if accelerator.is_main_process:\n",
    "                    save_checkpoint_custom(\n",
    "                        accelerator, accelerator.unwrap_model(unet), ema_unet,\n",
    "                        optimizer, lr_scheduler, global_step, current_epoch_for_tracking,\n",
    "                        checkpoint_dir_base, max_keep=max_keep_checkpoints\n",
    "                    )\n",
    "                    unet_ema_sample_model = UNet2DConditionModel.from_config(unet_for_sampling_config).to(accelerator.device)\n",
    "                    ema_unet.copy_to(unet_ema_sample_model.parameters())\n",
    "                    generate_samples(\n",
    "                        unet_ema_sample_model, vae, text_encoder, tokenizer,\n",
    "                        noise_scheduler, sampling_prompts, output_dir_base,\n",
    "                        global_step, accelerator.device, accelerator\n",
    "                    )\n",
    "                    del unet_ema_sample_model\n",
    "                    if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
    "            \n",
    "            if global_step >= max_train_steps:\n",
    "                break\n",
    "    \n",
    "    current_epoch_for_tracking +=1 # Increment epoch conceptually after iterating through dataloader\n",
    "    if global_step >= max_train_steps:\n",
    "        break\n",
    "\n",
    "progress_bar.close()\n",
    "accelerator.print(\"Training finished.\")\n",
    "\n",
    "# --- Salva modello finale (usually skipped for dummy data runs) ---\n",
    "if accelerator.is_main_process and not use_dummy_data:\n",
    "    accelerator.print(f\"Saving final model and components to {final_model_dir}\")\n",
    "    os.makedirs(final_model_dir, exist_ok=True)\n",
    "    unwrapped_unet = accelerator.unwrap_model(unet)\n",
    "    unwrapped_unet.save_pretrained(os.path.join(final_model_dir, \"unet\"))\n",
    "    final_ema_unet_model = UNet2DConditionModel.from_config(unwrapped_unet.config).to(accelerator.device)\n",
    "    ema_unet.copy_to(final_ema_unet_model.parameters())\n",
    "    final_ema_unet_model.save_pretrained(os.path.join(final_model_dir, \"ema_unet\"))\n",
    "    del final_ema_unet_model\n",
    "    vae.save_pretrained(os.path.join(final_model_dir, \"vae\"))\n",
    "    text_encoder.save_pretrained(os.path.join(final_model_dir, \"text_encoder\"))\n",
    "    tokenizer.save_pretrained(os.path.join(final_model_dir, \"tokenizer\"))\n",
    "    noise_scheduler.save_config(os.path.join(final_model_dir, \"scheduler\"))\n",
    "    accelerator.save_state(os.path.join(final_model_dir, \"accelerator_state\"))\n",
    "    torch.save(ema_unet.state_dict(), os.path.join(final_model_dir, \"ema_unet_final_state.pth\"))\n",
    "    final_custom_state = {\n",
    "        'global_step': global_step, 'epoch': current_epoch_for_tracking,\n",
    "        'wandb_run_id': wandb.run.id if use_wandb and wandb.run else None\n",
    "    }\n",
    "    torch.save(final_custom_state, os.path.join(final_model_dir, \"final_custom_training_state.pt\"))\n",
    "    if use_wandb and wandb.run:\n",
    "        try:\n",
    "            final_model_artifact = wandb.Artifact(\n",
    "                name=f\"{wandb_project_name.lower().replace(' ', '_')}-final_model\", type=\"model\",\n",
    "                description=f\"Final trained diffusion model components at step {global_step}.\",\n",
    "                metadata=wandb_config # wandb_config might not be fully defined if dummy_data was true and wandb init was skipped\n",
    "            )\n",
    "            final_model_artifact.add_dir(final_model_dir)\n",
    "            wandb.log_artifact(final_model_artifact)\n",
    "            accelerator.print(\"Final model saved as WandB artifact.\")\n",
    "        except Exception as e:\n",
    "            accelerator.print(f\"Failed to save model as WandB artifact: {e}\")\n",
    "elif accelerator.is_main_process and use_dummy_data:\n",
    "    accelerator.print(\"INFO: Dummy data run. Final model saving skipped.\")\n",
    "\n",
    "accelerator.end_training()\n",
    "accelerator.print(\"All components processed. Training script complete.\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa61ce0-851e-483f-ab19-00549763697a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d69f5b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install diffusers transformers datasets wandb Pillow tqdm xformers accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83535ee",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e46613",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7929a832",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import glob\n",
    "import shutil\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import math\n",
    "from torch.utils.data import DataLoader, IterableDataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image, ImageOps\n",
    "from tqdm.auto import tqdm\n",
    "import wandb\n",
    "import xformers # Added for XFORMERS\n",
    "\n",
    "# Hugging Face Libraries\n",
    "from diffusers import AutoencoderKL, UNet2DConditionModel, DDPMScheduler\n",
    "from diffusers.training_utils import EMAModel\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from datasets import load_dataset\n",
    "from accelerate import Accelerator, DistributedDataParallelKwargs\n",
    "from accelerate.utils import ProjectConfiguration, set_seed, LoggerType\n",
    "\n",
    "# --- Configuration (Hardcoded for simplicity) ---\n",
    "vae_model_id = \"stabilityai/sd-vae-ft-ema\"\n",
    "text_encoder_model_id = \"openai/clip-vit-large-patch14\"\n",
    "dataset_id = \"BLIP3o/BLIP3o-Pretrain-Short-Caption\"\n",
    "\n",
    "# Training Hyperparameters\n",
    "image_resolution = 512 #Il mio codice nella teoria fa il resize delle immagini in caso si vogliano utilizzare altri DS\n",
    "latent_resolution = image_resolution // 8 #image_resolution // vae_scale_factor (solitamente 8)\n",
    "train_batch_size = 16 # Batch per scheda video\n",
    "gradient_accumulation_steps = 8 # Effective batch size (GPU * acc_steps * batch_size)\n",
    "learning_rate = 1e-4 #std\n",
    "adam_beta1 = 0.9 #std\n",
    "adam_beta2 = 0.999 #std\n",
    "adam_weight_decay = 1e-2 #std\n",
    "adam_epsilon = 1e-08 #std\n",
    "max_grad_norm = 1.0 #Taglio i gradienti\n",
    "num_train_timesteps_scheduler = 1000 #Step del noise scheduler\n",
    "mixed_precision = \"bf16\" #Risparmia memoria e va pure piu veloce a fare i calcoli\n",
    "seed = 42 #Il numero perfetto come seed del random\n",
    "cfg_drop_probability = 0.1 # CFG Drop Probability\n",
    "max_train_steps = 30_000\n",
    "save_every_steps = max_train_steps // 100\n",
    "num_warmup_steps_ratio = 0.05 # 5% of max_train_steps for warmup\n",
    "num_warmup_steps = int(num_warmup_steps_ratio * max_train_steps)\n",
    "# --- Storage Path ---\n",
    "persistent_storage_mount_path = \"/workspace\"\n",
    "project_data_folder_name = \"PozzyDiffusion\"\n",
    "output_dir_base = os.path.join(persistent_storage_mount_path, project_data_folder_name)\n",
    "logging_dir = os.path.join(output_dir_base, \"logs\")\n",
    "checkpoint_dir_base = os.path.join(output_dir_base, \"checkpoints\")\n",
    "final_model_dir = os.path.join(output_dir_base, \"final_model\")\n",
    "max_keep_checkpoints = 2\n",
    "# WandB\n",
    "wandb_project_name = \"PozzyDiffusion_A40\"\n",
    "use_wandb = True\n",
    "# Hardcoded prompts per sampling\n",
    "sampling_prompts = [\n",
    "    \"a photo of an astronaut riding a horse on mars\",\n",
    "    \"a fantasy landscape with a castle and a dragon\",\n",
    "    \"a cyberpunk city at night with neon lights\",\n",
    "    \"a serene Japanese garden with cherry blossoms\",\n",
    "    \"a whimsical illustration of a cat playing a piano\"\n",
    "]\n",
    "num_inference_steps_sampling = 50 #Step che fa per ottenere l'img finale\n",
    "\n",
    "# --- Accelerator ---\n",
    "project_config = ProjectConfiguration(project_dir=output_dir_base, logging_dir=logging_dir)\n",
    "accelerator = Accelerator(\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    mixed_precision=mixed_precision,\n",
    "    log_with=\"wandb\" if use_wandb else None,\n",
    "    project_config=project_config,\n",
    ")\n",
    "# Imposta in modo globale il random seed cosi che posso riprodurre perfettamente tutto\n",
    "if seed is not None:\n",
    "    set_seed(seed)\n",
    "# Crea gia le cartelle\n",
    "if accelerator.is_main_process:\n",
    "    os.makedirs(output_dir_base, exist_ok=True)\n",
    "    os.makedirs(checkpoint_dir_base, exist_ok=True)\n",
    "    os.makedirs(final_model_dir, exist_ok=True)\n",
    "\n",
    "# --- Helper Functions ---\n",
    "def save_checkpoint_custom(accelerator, unet_original, ema_unet, optimizer, lr_scheduler, global_step, epoch, checkpoint_dir_base, filename_prefix=\"ckpt\", max_keep=2):\n",
    "    if accelerator.is_main_process:\n",
    "        os.makedirs(checkpoint_dir_base, exist_ok=True)\n",
    "        # Checkpoint path includes the step\n",
    "        checkpoint_name = f\"{filename_prefix}_step{global_step}\"\n",
    "        save_path = os.path.join(checkpoint_dir_base, checkpoint_name)\n",
    "        \n",
    "        accelerator.save_state(save_path) # Saves prepared models, optimizer, scheduler\n",
    "\n",
    "        # Save EMA model separately as it's not part of accelerator.prepare() directly\n",
    "        ema_save_path = os.path.join(save_path, \"ema_unet.pth\")\n",
    "        torch.save(ema_unet.state_dict(), ema_save_path)\n",
    "        \n",
    "        # Save additional training state (could be registered with accelerator too)\n",
    "        custom_state = {\n",
    "            'global_step': global_step,\n",
    "            'epoch': epoch,\n",
    "            'wandb_run_id': wandb.run.id if use_wandb and wandb.run else None\n",
    "        }\n",
    "        torch.save(custom_state, os.path.join(save_path, \"custom_training_state.pt\"))\n",
    "\n",
    "        accelerator.print(f\"Saved checkpoint: {save_path}\")\n",
    "\n",
    "        # Manage old checkpoints\n",
    "        checkpoints = sorted(\n",
    "            glob.glob(os.path.join(checkpoint_dir_base, f\"{filename_prefix}_step*\")),\n",
    "            key=lambda x: int(os.path.basename(x).split('step')[-1]) # Sort by step number\n",
    "        )\n",
    "        if len(checkpoints) > max_keep:\n",
    "            for old_ckpt_path in checkpoints[:-max_keep]:\n",
    "                accelerator.print(f\"Removing old checkpoint: {old_ckpt_path}\")\n",
    "                shutil.rmtree(old_ckpt_path) # Remove entire directory\n",
    "\n",
    "def load_latest_checkpoint_custom(accelerator, unet_original, ema_unet, optimizer, lr_scheduler, checkpoint_dir_base, filename_prefix=\"ckpt\"):\n",
    "    checkpoints = sorted(\n",
    "        glob.glob(os.path.join(checkpoint_dir_base, f\"{filename_prefix}_step*\")),\n",
    "        key=lambda x: int(os.path.basename(x).split('step')[-1]), # Sort by step number\n",
    "        reverse=True\n",
    "    )\n",
    "    if checkpoints:\n",
    "        latest_checkpoint_path = checkpoints[0]\n",
    "        accelerator.print(f\"Found checkpoint: {latest_checkpoint_path}\")\n",
    "        try:\n",
    "            accelerator.load_state(latest_checkpoint_path)\n",
    "            \n",
    "            ema_path = os.path.join(latest_checkpoint_path, \"ema_unet.pth\")\n",
    "            if os.path.exists(ema_path):\n",
    "                ema_unet.load_state_dict(torch.load(ema_path, map_location=\"cpu\"))\n",
    "                accelerator.print(\"EMA UNet state loaded.\")\n",
    "\n",
    "            custom_state_path = os.path.join(latest_checkpoint_path, \"custom_training_state.pt\")\n",
    "            if os.path.exists(custom_state_path):\n",
    "                custom_state = torch.load(custom_state_path, map_location=\"cpu\")\n",
    "                return custom_state.get('global_step', 0), custom_state.get('epoch', 0), custom_state.get('wandb_run_id')\n",
    "            else: # Fallback for older checkpoints perhaps, or if custom state saving failed\n",
    "                accelerator.print(\"Warning: custom_training_state.pt not found. Global step and epoch might not be accurate from checkpoint.\")\n",
    "                # Try to infer global_step from path if needed, though accelerator might handle optimizer step counts.\n",
    "                # For this setup, accelerator.load_state() should restore optimizer and scheduler internal steps.\n",
    "                # The main global_step is for tracking progress and naming.\n",
    "                parsed_step = int(os.path.basename(latest_checkpoint_path).split('step')[-1])\n",
    "                return parsed_step, 0, None # Assuming epoch 0 if not found\n",
    "\n",
    "        except Exception as e:\n",
    "            accelerator.print(f\"Error loading checkpoint {latest_checkpoint_path}: {e}. Starting fresh or from an earlier state if accelerator handled it.\")\n",
    "            return 0, 0, None # Could not load\n",
    "    return 0, 0, None # No checkpoint found\n",
    "\n",
    "def generate_samples(unet_model_for_sampling, vae, text_encoder, tokenizer_obj, noise_scheduler_obj, prompts, output_dir, global_step, device, accelerator_ref):\n",
    "    unet_model_for_sampling.eval() # Ensure the passed model is set to eval\n",
    "    vae.eval()\n",
    "    text_encoder.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, prompt in enumerate(prompts):\n",
    "            text_inputs = tokenizer_obj(prompt, padding=\"max_length\", max_length=tokenizer_obj.model_max_length, truncation=True, return_tensors=\"pt\")\n",
    "            # Move input_ids to the correct device\n",
    "            text_embeddings = text_encoder(text_inputs.input_ids.to(device))[0]\n",
    "            \n",
    "            # Use accelerator.device for latents\n",
    "            latents_shape = (1, unet_model_for_sampling.config.in_channels, latent_resolution, latent_resolution)\n",
    "            \n",
    "            # --- FIXED LINE ---\n",
    "            # The generator must be created on the same device as the target tensor.\n",
    "            # torch.manual_seed() creates a CPU generator by default.\n",
    "            # The fix is to instantiate a generator on the correct device.\n",
    "            generator = torch.Generator(device=device).manual_seed(seed + i) if seed is not None else None\n",
    "            latents = torch.randn(latents_shape, device=device, generator=generator) # Add seed for reproducibility of samples\n",
    "            \n",
    "            noise_scheduler_obj.set_timesteps(num_inference_steps_sampling) # Use configured number of steps\n",
    "            \n",
    "            for t in tqdm(noise_scheduler_obj.timesteps, desc=f\"Sampling for prompt {i+1}\", disable=not accelerator_ref.is_main_process):\n",
    "                # scale the model input\n",
    "                latent_model_input = noise_scheduler_obj.scale_model_input(latents, t)\n",
    "                noise_pred = unet_model_for_sampling(latent_model_input, t, encoder_hidden_states=text_embeddings).sample\n",
    "                latents = noise_scheduler_obj.step(noise_pred, t, latents).prev_sample\n",
    "            \n",
    "            latents = 1 / vae.config.scaling_factor * latents\n",
    "            image = vae.decode(latents).sample\n",
    "            image = (image / 2 + 0.5).clamp(0, 1) # Denormalize\n",
    "            image = image.cpu().permute(0, 2, 3, 1).numpy()[0] # (H, W, C)\n",
    "            image = Image.fromarray((image * 255).round().astype(\"uint8\"))\n",
    "            \n",
    "            if accelerator_ref.is_main_process:\n",
    "                sample_output_dir = os.path.join(output_dir, \"samples\", f\"step_{global_step}\")\n",
    "                os.makedirs(sample_output_dir, exist_ok=True)\n",
    "                img_path = os.path.join(sample_output_dir, f\"prompt_{i+1}_seed{seed+i if seed is not None else 'rand'}.png\")\n",
    "                image.save(img_path)\n",
    "                accelerator_ref.print(f\"Generated sample for prompt '{prompt}' saved to {img_path}\")\n",
    "    \n",
    "    unet_model_for_sampling.train() # Set back to train if it was the main model (though here it's a copy)\n",
    "\n",
    "# --- Inizializza i modelli ---\n",
    "accelerator.print(\"Loading VAE...\")\n",
    "vae = AutoencoderKL.from_pretrained(vae_model_id)\n",
    "accelerator.print(\"Loading Text Encoder & Tokenizer...\")\n",
    "tokenizer = CLIPTokenizer.from_pretrained(text_encoder_model_id)\n",
    "text_encoder = CLIPTextModel.from_pretrained(text_encoder_model_id)\n",
    "accelerator.print(\"Initializing UNet...\")\n",
    "# Struttura originale su cui si basera l'EMA\n",
    "original_unet = UNet2DConditionModel(\n",
    "    sample_size=latent_resolution, in_channels=vae.config.latent_channels, out_channels=vae.config.latent_channels,\n",
    "    layers_per_block=2, block_out_channels=(256, 512, 768, 1024), # Example, adjust as needed\n",
    "    down_block_types=(\"CrossAttnDownBlock2D\", \"CrossAttnDownBlock2D\", \"CrossAttnDownBlock2D\", \"DownBlock2D\"),\n",
    "    up_block_types=(\"UpBlock2D\", \"CrossAttnUpBlock2D\", \"CrossAttnUpBlock2D\", \"CrossAttnUpBlock2D\"),\n",
    "    cross_attention_dim=text_encoder.config.hidden_size,\n",
    ")\n",
    "#Conto i parametri\n",
    "unet_param_count = sum(p.numel() for p in original_unet.parameters() if p.requires_grad)\n",
    "accelerator.print(f\"UNet initialized with {unet_param_count / 1e6:.2f}M parameters.\")\n",
    "#Inizializza EMA model\n",
    "ema_unet = EMAModel(original_unet.parameters(), model_cls=UNet2DConditionModel, model_config=original_unet.config)\n",
    "ema_unet.to(accelerator.device) #E lo butto sulla GPU\n",
    "#Butto anche gli altri 2 coglioni sulla gpu, ma non allenandoli metto require_grad a false per risparmiare memoria\n",
    "vae.to(accelerator.device).eval().requires_grad_(False)\n",
    "text_encoder.to(accelerator.device).eval().requires_grad_(False)\n",
    "\n",
    "#Embedding vuoto per CFG invece di calcolarlo ogni volta come uno stupido\n",
    "with torch.no_grad():\n",
    "    uncond_tokens = tokenizer([\"\"], padding=\"max_length\", max_length=tokenizer.model_max_length, return_tensors=\"pt\").input_ids\n",
    "    uncond_embeddings = text_encoder(uncond_tokens.to(accelerator.device))[0]\n",
    "\n",
    "#Provo ad utilizzare XFormers per risparmiare memoria\n",
    "if xformers.version:\n",
    "    try:\n",
    "        original_unet.enable_xformers_memory_efficient_attention()\n",
    "        accelerator.print(\"XFormers memory efficient attention enabled for UNet.\")\n",
    "    except Exception as e:\n",
    "        accelerator.print(f\"Could not enable xformers memory efficient attention: {e}\")\n",
    "else:\n",
    "    accelerator.print(\"XFormers not available or not installed. Skipping memory efficient attention.\") #Cacca\n",
    "#Qua prima compilavo, ora non compilo piu il modello\n",
    "compiled_unet_object = None #Se non compilo lascio a none\n",
    "accelerator.print(\"Skipping UNet compilation to avoid potential CUDAGraph issues.\") # Updated message\n",
    "unet_to_prepare = original_unet # Directly use the original, uncompiled UNet\n",
    "\n",
    "# --- Noise Scheduler ---\n",
    "noise_scheduler = DDPMScheduler(\n",
    "    num_train_timesteps=num_train_timesteps_scheduler,\n",
    "    beta_schedule=\"scaled_linear\",\n",
    "    prediction_type=\"v_prediction\" # Switched to v-prediction\n",
    ")\n",
    "\n",
    "# --- Optimizer ---\n",
    "optimizer = AdamW(\n",
    "    unet_to_prepare.parameters(), # Optimize parameters of the (potentially compiled) UNet\n",
    "    lr=learning_rate, betas=(adam_beta1, adam_beta2),\n",
    "    weight_decay=adam_weight_decay, eps=adam_epsilon,\n",
    "    # Fused AdamW is often handled automatically by PyTorch/Accelerate or can be enabled if available\n",
    "    # fused=True if accelerator.device.type == 'cuda' and mixed_precision in [\"fp16\", \"bf16\"] else False # Be cautious with fused and compilation\n",
    ")\n",
    "\n",
    "# --- Learning Rate Scheduler ---\n",
    "def lr_lambda_cosine(current_step: int):\n",
    "    if num_warmup_steps > 0 and current_step < num_warmup_steps:\n",
    "        return float(current_step) / float(max(1, num_warmup_steps))\n",
    "    denominator = float(max(1, max_train_steps - num_warmup_steps))\n",
    "    progress = float(current_step - num_warmup_steps) / denominator\n",
    "    return max(0.0, 0.5 * (1.0 + math.cos(math.pi * progress)))\n",
    "\n",
    "lr_scheduler = LambdaLR(optimizer, lr_lambda_cosine)\n",
    "\n",
    "\n",
    "# --- Sistemo img ---\n",
    "image_transforms = transforms.Compose([\n",
    "    transforms.Resize((image_resolution, image_resolution), interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5]), # Normalize to [-1, 1]\n",
    "])\n",
    "#Preprocessing\n",
    "def preprocess_function(examples, tokenizer_obj, image_transforms_fn, accelerator_ref):\n",
    "    prompts, pil_images = examples[\"txt\"], examples[\"jpg\"]\n",
    "    processed_images, valid_prompts = [], []\n",
    "    for idx, (img_pil, prompt_text) in enumerate(zip(pil_images, prompts)):\n",
    "        try:\n",
    "            if img_pil is None:\n",
    "                if accelerator_ref.is_main_process: accelerator_ref.print(f\"Warning: Found None image for prompt: {prompt_text}. Skipping.\")\n",
    "                continue\n",
    "            img = img_pil.convert(\"RGB\")\n",
    "            img = ImageOps.exif_transpose(img) # Handle EXIF orientation\n",
    "            processed_images.append(image_transforms_fn(img))\n",
    "            valid_prompts.append(prompt_text)\n",
    "        except Exception as e:\n",
    "            if accelerator_ref.is_main_process: # Log only on main process to avoid spam\n",
    "                accelerator_ref.print(f\"Warning: Skipping an image/prompt due to error: {e}. Prompt: '{prompt_text}'. Image index in batch: {idx}\")\n",
    "            continue\n",
    "    \n",
    "    if not processed_images: # All images in this chunk were bad\n",
    "        return None\n",
    "        \n",
    "    text_inputs = tokenizer_obj(valid_prompts, padding=\"max_length\", max_length=tokenizer_obj.model_max_length, truncation=True, return_tensors=\"pt\")\n",
    "    return {\"pixel_values\": torch.stack(processed_images), \"input_ids\": text_inputs.input_ids}\n",
    "\n",
    "#Streaming DS\n",
    "class StreamingImageTextDataset(IterableDataset):\n",
    "    def __init__(self, dataset_id, split, transform_fn, tokenizer_obj, image_transforms_fn, processing_chunk_size, accelerator_ref):\n",
    "        self.dataset = load_dataset(dataset_id, split=split, streaming=True)\n",
    "        self.transform_fn = transform_fn\n",
    "        self.tokenizer_obj = tokenizer_obj\n",
    "        self.image_transforms_fn = image_transforms_fn\n",
    "        self.processing_chunk_size = processing_chunk_size\n",
    "        self.accelerator_ref = accelerator_ref # For logging inside preprocess\n",
    "\n",
    "    def __iter__(self):\n",
    "        buffer = []\n",
    "        for example in self.dataset:\n",
    "            # Ensure 'jpg' and 'txt' keys exist and are not None\n",
    "            if example.get(\"jpg\") is not None and example.get(\"txt\") is not None:\n",
    "                buffer.append({\"jpg\": example[\"jpg\"], \"txt\": example[\"txt\"]})\n",
    "            else:\n",
    "                if self.accelerator_ref.is_main_process:\n",
    "                    self.accelerator_ref.print(f\"Warning: Skipping example due to missing 'jpg' or 'txt' field: {example.get('txt', 'N/A')}\")\n",
    "                continue\n",
    "\n",
    "            if len(buffer) == self.processing_chunk_size:\n",
    "                processed_batch = self.transform_fn(\n",
    "                    {\"jpg\": [i[\"jpg\"] for i in buffer], \"txt\": [i[\"txt\"] for i in buffer]},\n",
    "                    self.tokenizer_obj, self.image_transforms_fn, self.accelerator_ref\n",
    "                )\n",
    "                if processed_batch:\n",
    "                    for i in range(processed_batch[\"pixel_values\"].size(0)):\n",
    "                        yield {\"pixel_values\": processed_batch[\"pixel_values\"][i], \"input_ids\": processed_batch[\"input_ids\"][i]}\n",
    "                buffer = []\n",
    "        \n",
    "        # Process any remaining items in the buffer\n",
    "        if buffer:\n",
    "            processed_batch = self.transform_fn(\n",
    "                {\"jpg\": [i[\"jpg\"] for i in buffer], \"txt\": [i[\"txt\"] for i in buffer]},\n",
    "                self.tokenizer_obj, self.image_transforms_fn, self.accelerator_ref\n",
    "            )\n",
    "            if processed_batch:\n",
    "                for i in range(processed_batch[\"pixel_values\"].size(0)):\n",
    "                    yield {\"pixel_values\": processed_batch[\"pixel_values\"][i], \"input_ids\": processed_batch[\"input_ids\"][i]}\n",
    "\n",
    "accelerator.print(\"Setting up dataset stream...\")\n",
    "#Num di sample passati insieme alla funzione di preprocessing\n",
    "#Evito di chiamare la stessa funzione piu volte cosi\n",
    "transform_processing_chunk_size = 1\n",
    "#Numero di thread che svolgono questo lavoro di merda\n",
    "num_dataloader_workers = 8\n",
    "\n",
    "#Bon, metto tutto insieme\n",
    "train_dataset = StreamingImageTextDataset(\n",
    "    dataset_id=dataset_id, split=\"train\", \n",
    "    transform_fn=preprocess_function, \n",
    "    tokenizer_obj=tokenizer, \n",
    "    image_transforms_fn=image_transforms,\n",
    "    processing_chunk_size=transform_processing_chunk_size,\n",
    "    accelerator_ref=accelerator\n",
    ")\n",
    "#Sistemo tutto\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, batch_size=train_batch_size, \n",
    "    num_workers=num_dataloader_workers, \n",
    "    pin_memory=True if num_dataloader_workers > 0 else False, # Pin memory if using workers\n",
    "    persistent_workers=True if num_dataloader_workers > 0 else False\n",
    ")\n",
    "#Daje, ci si prepara\n",
    "unet, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
    "    unet_to_prepare, optimizer, train_dataloader, lr_scheduler\n",
    ")\n",
    "\n",
    "# --- Resume from Checkpoint perche non so se la macchina a meta schioppa ---\n",
    "global_step, start_epoch, resume_wandb_id = 0, 0, None\n",
    "resume_wandb_id = None\n",
    "try:\n",
    "    global_step, start_epoch, resume_wandb_id = load_latest_checkpoint_custom(\n",
    "        accelerator, accelerator.unwrap_model(unet), ema_unet, optimizer, lr_scheduler, checkpoint_dir_base\n",
    "    )\n",
    "    if global_step > 0:\n",
    "        accelerator.print(f\"Resumed training from step {global_step}, epoch {start_epoch}.\")\n",
    "    else:\n",
    "        accelerator.print(\"Starting training from scratch or no compatible checkpoint found.\")\n",
    "except Exception as e:\n",
    "    accelerator.print(f\"Could not load checkpoint. Starting from scratch. Error: {e}\")\n",
    "    global_step, start_epoch, resume_wandb_id = 0, 0, None\n",
    "ema_unet.to(accelerator.device)\n",
    "\n",
    "# --- WandB Initialization ---\n",
    "if use_wandb and accelerator.is_main_process:\n",
    "    wandb_config = {\n",
    "        \"learning_rate\": learning_rate, \"train_batch_size\": train_batch_size,\n",
    "        \"gradient_accumulation_steps\": gradient_accumulation_steps,\n",
    "        \"effective_batch_size\": train_batch_size * accelerator.num_processes * gradient_accumulation_steps,\n",
    "        \"image_resolution\": image_resolution, \"unet_params_M\": unet_param_count / 1e6,\n",
    "        \"vae_model_id\": vae_model_id, \"text_encoder_model_id\": text_encoder_model_id,\n",
    "        \"dataset_id\": dataset_id, \"max_train_steps\": max_train_steps,\n",
    "        \"num_warmup_steps\": num_warmup_steps, \"mixed_precision\": mixed_precision,\n",
    "        \"torch_compile_mode\": \"reduce-overhead\" if compiled_unet_object is not None else \"None\",\n",
    "        \"v_prediction\": True, \"cfg_drop_probability\": cfg_drop_probability, \"seed\": seed,\n",
    "    }\n",
    "    #Resume se ce un checkpoint\n",
    "    run_id_to_resume = resume_wandb_id if global_step > 0 and resume_wandb_id else None\n",
    "    \n",
    "    accelerator.init_trackers(\n",
    "        project_name=wandb_project_name,\n",
    "        config=wandb_config,\n",
    "        init_kwargs={\"wandb\": {\"id\": run_id_to_resume, \"resume\": \"allow\"}}\n",
    "    )\n",
    "    if run_id_to_resume: accelerator.print(f\"Resuming WandB run with ID: {run_id_to_resume}\")\n",
    "    elif global_step > 0: accelerator.print(\"Warning: Resuming training but no wandb_run_id found in checkpoint for WandB.\")\n",
    "\n",
    "\n",
    "# --- Training Loop ---\n",
    "accelerator.print(f\"Starting training. Target steps: {max_train_steps}. Current step: {global_step}. Warmup steps: {num_warmup_steps}.\")\n",
    "progress_bar = tqdm(initial=global_step, total=max_train_steps, desc=\"Training Steps\", disable=not accelerator.is_main_process)\n",
    "current_epoch_for_tracking = start_epoch # For saving in checkpoint\n",
    "\n",
    "unet_for_sampling_config = accelerator.unwrap_model(unet).config\n",
    "\n",
    "# Main training loop\n",
    "while global_step < max_train_steps:\n",
    "    unet.train()\n",
    "    for batch_idx, batch in enumerate(train_dataloader):\n",
    "        if global_step >= max_train_steps:\n",
    "            break\n",
    "\n",
    "        with accelerator.accumulate(unet): #Gradient acc\n",
    "            pixel_values = batch[\"pixel_values\"]\n",
    "            input_ids = batch[\"input_ids\"]\n",
    "            bsz = pixel_values.shape[0]\n",
    "\n",
    "            with torch.no_grad():\n",
    "                latents = vae.encode(pixel_values).latent_dist.sample() * vae.config.scaling_factor\n",
    "                encoder_hidden_states = text_encoder(input_ids)[0]\n",
    "\n",
    "            # Classifier-Free Guidance\n",
    "            mask = torch.rand(bsz, device=accelerator.device) < cfg_drop_probability\n",
    "            if mask.any():\n",
    "                 encoder_hidden_states[mask] = uncond_embeddings.expand(mask.sum(), -1, -1)\n",
    "\n",
    "            noise = torch.randn_like(latents)\n",
    "            timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (bsz,), device=latents.device).long()\n",
    "            noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
    "            \n",
    "            #V-prediction invece di image o noise prediction (via di mezzo, dovrebbe funzionare anche se male)\n",
    "            target_velocity = noise_scheduler.get_velocity(latents, noise, timesteps)\n",
    "\n",
    "            #Mixed precision\n",
    "            model_pred = unet(noisy_latents, timesteps, encoder_hidden_states).sample\n",
    "            loss = F.mse_loss(model_pred.float(), target_velocity.float(), reduction=\"mean\")\n",
    "\n",
    "            accelerator.backward(loss)\n",
    "            if accelerator.sync_gradients: # Only clip when gradients are synced (after accumulation)\n",
    "                accelerator.clip_grad_norm_(unet.parameters(), max_grad_norm)\n",
    "            \n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        #Usa param della Unet originale\n",
    "        if accelerator.sync_gradients: #Update EMA unet\n",
    "            ema_unet.step(accelerator.unwrap_model(unet).parameters())\n",
    "\n",
    "        if accelerator.sync_gradients: #Progress barrrrrrr\n",
    "            progress_bar.update(1)\n",
    "            global_step += 1\n",
    "            \n",
    "            if global_step % 100 == 0:\n",
    "                current_lr = optimizer.param_groups[0]['lr']\n",
    "                log_data = {\"train_loss\": loss.item(), \"learning_rate\": current_lr, \"global_step\": global_step}\n",
    "                accelerator.log(log_data, step=global_step)\n",
    "                progress_bar.set_postfix({\"loss\": f\"{loss.item():.4f}\", \"lr\": f\"{current_lr:.2e}\"})\n",
    "\n",
    "                #Salva\n",
    "            if global_step > 0 and global_step % save_every_steps == 0 and save_every_steps > 0:\n",
    "                if accelerator.is_main_process:\n",
    "                    save_checkpoint_custom(\n",
    "                        accelerator,\n",
    "                        accelerator.unwrap_model(unet), #original model\n",
    "                        ema_unet, #EMA model instance\n",
    "                        optimizer, #prepared optimizer\n",
    "                        lr_scheduler, #prepared scheduler\n",
    "                        global_step, #step a cui siamo arrivati\n",
    "                        current_epoch_for_tracking, #book-keeping, non tanto necessario\n",
    "                        checkpoint_dir_base,\n",
    "                        max_keep=max_keep_checkpoints\n",
    "                    )\n",
    "                    \n",
    "                    #Genera qualche img per i mie occhietti curiosi\n",
    "                    unet_ema_sample_model = UNet2DConditionModel.from_config(unet_for_sampling_config).to(accelerator.device)\n",
    "                    ema_unet.copy_to(unet_ema_sample_model.parameters()) # Copy EMA params to this new model\n",
    "                    \n",
    "                    generate_samples(\n",
    "                        unet_ema_sample_model, vae, text_encoder, tokenizer,\n",
    "                        noise_scheduler, sampling_prompts, output_dir_base,\n",
    "                        global_step, accelerator.device, accelerator\n",
    "                    )\n",
    "                    del unet_ema_sample_model #Libera mem\n",
    "                    if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
    "            \n",
    "            if global_step >= max_train_steps:\n",
    "                break\n",
    "    \n",
    "    #Esce quando gli step sono stati fatti\n",
    "    if global_step >= max_train_steps:\n",
    "        break\n",
    "\n",
    "\n",
    "progress_bar.close()\n",
    "accelerator.print(\"Training finished.\")\n",
    "\n",
    "# --- Salva modello finale ---\n",
    "if accelerator.is_main_process:\n",
    "    accelerator.print(f\"Saving final model and components to {final_model_dir}\")\n",
    "    os.makedirs(final_model_dir, exist_ok=True)\n",
    "    unwrapped_unet = accelerator.unwrap_model(unet)\n",
    "    unwrapped_unet.save_pretrained(os.path.join(final_model_dir, \"unet\"))\n",
    "    final_ema_unet_model = UNet2DConditionModel.from_config(unwrapped_unet.config).to(accelerator.device)\n",
    "    ema_unet.copy_to(final_ema_unet_model.parameters())\n",
    "    final_ema_unet_model.save_pretrained(os.path.join(final_model_dir, \"ema_unet\"))\n",
    "    del final_ema_unet_model\n",
    "    vae.save_pretrained(os.path.join(final_model_dir, \"vae\"))\n",
    "    text_encoder.save_pretrained(os.path.join(final_model_dir, \"text_encoder\"))\n",
    "    tokenizer.save_pretrained(os.path.join(final_model_dir, \"tokenizer\"))\n",
    "    noise_scheduler.save_config(os.path.join(final_model_dir, \"scheduler\")) # Saves config.json\n",
    "    accelerator.save_state(os.path.join(final_model_dir, \"accelerator_state\"))\n",
    "    torch.save(ema_unet.state_dict(), os.path.join(final_model_dir, \"ema_unet_final_state.pth\"))\n",
    "    final_custom_state = {\n",
    "        'global_step': global_step, 'epoch': current_epoch_for_tracking,\n",
    "        'wandb_run_id': wandb.run.id if use_wandb and wandb.run else None\n",
    "    }\n",
    "    torch.save(final_custom_state, os.path.join(final_model_dir, \"final_custom_training_state.pt\"))\n",
    "    if use_wandb and wandb.run:\n",
    "        try:\n",
    "            final_model_artifact = wandb.Artifact(\n",
    "                name=f\"{wandb_project_name.lower().replace(' ', '_')}-final_model\",\n",
    "                type=\"model\",\n",
    "                description=f\"Final trained diffusion model components at step {global_step}.\",\n",
    "                metadata=wandb_config\n",
    "            )\n",
    "            final_model_artifact.add_dir(final_model_dir)\n",
    "            wandb.log_artifact(final_model_artifact)\n",
    "            accelerator.print(\"Final model saved as WandB artifact.\")\n",
    "        except Exception as e:\n",
    "            accelerator.print(f\"Failed to save model as WandB artifact: {e}\")\n",
    "\n",
    "\n",
    "accelerator.end_training()\n",
    "accelerator.print(\"All components saved. Training script complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b656f035",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#Mega test se la pipeline e il problema...\n",
    "#Non e il problema...\n",
    "\"\"\"\n",
    "import os\n",
    "import io\n",
    "import glob\n",
    "import shutil\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import math\n",
    "from torch.utils.data import DataLoader, IterableDataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image, ImageOps\n",
    "from tqdm.auto import tqdm\n",
    "import wandb\n",
    "import xformers # Added for XFORMERS\n",
    "\n",
    "# Hugging Face Libraries\n",
    "from diffusers import AutoencoderKL, UNet2DConditionModel, DDPMScheduler\n",
    "from diffusers.training_utils import EMAModel\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from datasets import load_dataset\n",
    "from accelerate import Accelerator, DistributedDataParallelKwargs\n",
    "from accelerate.utils import ProjectConfiguration, set_seed, LoggerType\n",
    "\n",
    "# --- Configuration (Hardcoded for simplicity) ---\n",
    "use_dummy_data = True  # <<< SET THIS TO True TO USE DUMMY DATA FOR BOTTLENECK TESTING\n",
    "# If True, the script will bypass dataset loading and use randomly generated tensors.\n",
    "\n",
    "vae_model_id = \"stabilityai/sd-vae-ft-ema\"\n",
    "text_encoder_model_id = \"openai/clip-vit-large-patch14\"\n",
    "dataset_id = \"BLIP3o/BLIP3o-Pretrain-Short-Caption\" # Used only if use_dummy_data is False\n",
    "\n",
    "# Training Hyperparameters\n",
    "image_resolution = 512\n",
    "latent_resolution = image_resolution // 8\n",
    "train_batch_size = 16\n",
    "gradient_accumulation_steps = 8\n",
    "learning_rate = 1e-4\n",
    "adam_beta1 = 0.9\n",
    "adam_beta2 = 0.999\n",
    "adam_weight_decay = 1e-2\n",
    "adam_epsilon = 1e-08\n",
    "max_grad_norm = 1.0\n",
    "num_train_timesteps_scheduler = 1000\n",
    "mixed_precision = \"bf16\"\n",
    "seed = 42\n",
    "cfg_drop_probability = 0.1\n",
    "max_train_steps = 30_000\n",
    "save_every_steps = max_train_steps // 100\n",
    "num_warmup_steps_ratio = 0.05\n",
    "num_warmup_steps = int(num_warmup_steps_ratio * max_train_steps)\n",
    "# --- Storage Path ---\n",
    "persistent_storage_mount_path = \"/workspace\"\n",
    "project_data_folder_name = \"PozzyDiffusion\"\n",
    "output_dir_base = os.path.join(persistent_storage_mount_path, project_data_folder_name)\n",
    "logging_dir = os.path.join(output_dir_base, \"logs\")\n",
    "checkpoint_dir_base = os.path.join(output_dir_base, \"checkpoints\")\n",
    "final_model_dir = os.path.join(output_dir_base, \"final_model\")\n",
    "max_keep_checkpoints = 2\n",
    "# WandB\n",
    "wandb_project_name = \"PozzyDiffusion_A40\"\n",
    "use_wandb = False # Set to False if you want to run dummy data test without WandB logging\n",
    "# Hardcoded prompts per sampling\n",
    "sampling_prompts = [\n",
    "    \"a photo of an astronaut riding a horse on mars\",\n",
    "    \"a fantasy landscape with a castle and a dragon\",\n",
    "    \"a cyberpunk city at night with neon lights\",\n",
    "    \"a serene Japanese garden with cherry blossoms\",\n",
    "    \"a whimsical illustration of a cat playing a piano\"\n",
    "]\n",
    "num_inference_steps_sampling = 50\n",
    "\n",
    "# --- Accelerator ---\n",
    "project_config = ProjectConfiguration(project_dir=output_dir_base, logging_dir=logging_dir)\n",
    "accelerator = Accelerator(\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    mixed_precision=mixed_precision,\n",
    "    log_with=\"wandb\" if use_wandb and not use_dummy_data else None, # Optionally disable wandb for dummy runs\n",
    "    project_config=project_config,\n",
    ")\n",
    "if seed is not None:\n",
    "    set_seed(seed)\n",
    "if accelerator.is_main_process:\n",
    "    os.makedirs(output_dir_base, exist_ok=True)\n",
    "    os.makedirs(checkpoint_dir_base, exist_ok=True)\n",
    "    os.makedirs(final_model_dir, exist_ok=True)\n",
    "\n",
    "# --- Helper Functions ---\n",
    "def save_checkpoint_custom(accelerator, unet_original, ema_unet, optimizer, lr_scheduler, global_step, epoch, checkpoint_dir_base, filename_prefix=\"ckpt\", max_keep=2):\n",
    "    if accelerator.is_main_process:\n",
    "        os.makedirs(checkpoint_dir_base, exist_ok=True)\n",
    "        checkpoint_name = f\"{filename_prefix}_step{global_step}\"\n",
    "        save_path = os.path.join(checkpoint_dir_base, checkpoint_name)\n",
    "        accelerator.save_state(save_path)\n",
    "        ema_save_path = os.path.join(save_path, \"ema_unet.pth\")\n",
    "        torch.save(ema_unet.state_dict(), ema_save_path)\n",
    "        custom_state = {\n",
    "            'global_step': global_step, 'epoch': epoch,\n",
    "            'wandb_run_id': wandb.run.id if use_wandb and wandb.run and not use_dummy_data else None\n",
    "        }\n",
    "        torch.save(custom_state, os.path.join(save_path, \"custom_training_state.pt\"))\n",
    "        accelerator.print(f\"Saved checkpoint: {save_path}\")\n",
    "        checkpoints = sorted(\n",
    "            glob.glob(os.path.join(checkpoint_dir_base, f\"{filename_prefix}_step*\")),\n",
    "            key=lambda x: int(os.path.basename(x).split('step')[-1])\n",
    "        )\n",
    "        if len(checkpoints) > max_keep:\n",
    "            for old_ckpt_path in checkpoints[:-max_keep]:\n",
    "                accelerator.print(f\"Removing old checkpoint: {old_ckpt_path}\")\n",
    "                shutil.rmtree(old_ckpt_path)\n",
    "\n",
    "def load_latest_checkpoint_custom(accelerator, unet_original, ema_unet, optimizer, lr_scheduler, checkpoint_dir_base, filename_prefix=\"ckpt\"):\n",
    "    checkpoints = sorted(\n",
    "        glob.glob(os.path.join(checkpoint_dir_base, f\"{filename_prefix}_step*\")),\n",
    "        key=lambda x: int(os.path.basename(x).split('step')[-1]),\n",
    "        reverse=True\n",
    "    )\n",
    "    if checkpoints:\n",
    "        latest_checkpoint_path = checkpoints[0]\n",
    "        accelerator.print(f\"Found checkpoint: {latest_checkpoint_path}\")\n",
    "        try:\n",
    "            accelerator.load_state(latest_checkpoint_path)\n",
    "            ema_path = os.path.join(latest_checkpoint_path, \"ema_unet.pth\")\n",
    "            if os.path.exists(ema_path):\n",
    "                ema_unet.load_state_dict(torch.load(ema_path, map_location=\"cpu\"))\n",
    "                accelerator.print(\"EMA UNet state loaded.\")\n",
    "            custom_state_path = os.path.join(latest_checkpoint_path, \"custom_training_state.pt\")\n",
    "            if os.path.exists(custom_state_path):\n",
    "                custom_state = torch.load(custom_state_path, map_location=\"cpu\")\n",
    "                return custom_state.get('global_step', 0), custom_state.get('epoch', 0), custom_state.get('wandb_run_id')\n",
    "            else:\n",
    "                parsed_step = int(os.path.basename(latest_checkpoint_path).split('step')[-1])\n",
    "                return parsed_step, 0, None\n",
    "        except Exception as e:\n",
    "            accelerator.print(f\"Error loading checkpoint {latest_checkpoint_path}: {e}. Starting fresh.\")\n",
    "            return 0, 0, None\n",
    "    return 0, 0, None\n",
    "\n",
    "def generate_samples(unet_model_for_sampling, vae, text_encoder, tokenizer_obj, noise_scheduler_obj, prompts, output_dir, global_step, device, accelerator_ref):\n",
    "    # This function might be less meaningful with dummy data, but can still run.\n",
    "    # Consider skipping if use_dummy_data is True and samples are not needed for the test.\n",
    "    if use_dummy_data and accelerator_ref.is_main_process:\n",
    "        accelerator_ref.print(\"Skipping sample generation during dummy data run.\")\n",
    "        return\n",
    "\n",
    "    unet_model_for_sampling.eval()\n",
    "    vae.eval()\n",
    "    text_encoder.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, prompt in enumerate(prompts):\n",
    "            text_inputs = tokenizer_obj(prompt, padding=\"max_length\", max_length=tokenizer_obj.model_max_length, truncation=True, return_tensors=\"pt\")\n",
    "            text_embeddings = text_encoder(text_inputs.input_ids.to(device))[0]\n",
    "            latents_shape = (1, unet_model_for_sampling.config.in_channels, latent_resolution, latent_resolution)\n",
    "            latents = torch.randn(latents_shape, device=device, generator=torch.manual_seed(seed + i) if seed is not None else None)\n",
    "            noise_scheduler_obj.set_timesteps(num_inference_steps_sampling)\n",
    "            for t in tqdm(noise_scheduler_obj.timesteps, desc=f\"Sampling for prompt {i+1}\", disable=not accelerator_ref.is_main_process):\n",
    "                latent_model_input = noise_scheduler_obj.scale_model_input(latents, t)\n",
    "                noise_pred = unet_model_for_sampling(latent_model_input, t, encoder_hidden_states=text_embeddings).sample\n",
    "                latents = noise_scheduler_obj.step(noise_pred, t, latents).prev_sample\n",
    "            latents = 1 / vae.config.scaling_factor * latents\n",
    "            image = vae.decode(latents).sample\n",
    "            image = (image / 2 + 0.5).clamp(0, 1)\n",
    "            image = image.cpu().permute(0, 2, 3, 1).numpy()[0]\n",
    "            image = Image.fromarray((image * 255).round().astype(\"uint8\"))\n",
    "            if accelerator_ref.is_main_process:\n",
    "                sample_output_dir = os.path.join(output_dir, \"samples\", f\"step_{global_step}\")\n",
    "                os.makedirs(sample_output_dir, exist_ok=True)\n",
    "                img_path = os.path.join(sample_output_dir, f\"prompt_{i+1}_seed{seed+i if seed is not None else 'rand'}.png\")\n",
    "                image.save(img_path)\n",
    "                accelerator_ref.print(f\"Generated sample for prompt '{prompt}' saved to {img_path}\")\n",
    "    unet_model_for_sampling.train()\n",
    "\n",
    "\n",
    "# --- Dummy Data Iterable Dataset ---\n",
    "class DummyIterableDataset(IterableDataset):\n",
    "    def __init__(self, image_res, tokenizer_max_len, pixel_dtype=torch.float32, input_id_dtype=torch.long):\n",
    "        super().__init__()\n",
    "        self.image_resolution = image_res\n",
    "        self.tokenizer_max_length = tokenizer_max_len\n",
    "        self.pixel_dtype = pixel_dtype\n",
    "        self.input_id_dtype = input_id_dtype\n",
    "\n",
    "    def __iter__(self):\n",
    "        while True:\n",
    "            dummy_pixel_values = torch.randn(\n",
    "                3, self.image_resolution, self.image_resolution, dtype=self.pixel_dtype\n",
    "            )\n",
    "            dummy_input_ids = torch.ones(\n",
    "                self.tokenizer_max_length, dtype=self.input_id_dtype\n",
    "            )\n",
    "            # You could use torch.randint for more varied input_ids if needed:\n",
    "            # dummy_input_ids = torch.randint(0, 30000, (self.tokenizer_max_length,), dtype=self.input_id_dtype) # Assumes vocab_size > 30000\n",
    "            yield {\"pixel_values\": dummy_pixel_values, \"input_ids\": dummy_input_ids}\n",
    "\n",
    "# --- Inizializza i modelli ---\n",
    "accelerator.print(\"Loading VAE...\")\n",
    "vae = AutoencoderKL.from_pretrained(vae_model_id)\n",
    "accelerator.print(\"Loading Text Encoder & Tokenizer...\")\n",
    "tokenizer = CLIPTokenizer.from_pretrained(text_encoder_model_id)\n",
    "text_encoder = CLIPTextModel.from_pretrained(text_encoder_model_id)\n",
    "accelerator.print(\"Initializing UNet...\")\n",
    "original_unet = UNet2DConditionModel(\n",
    "    sample_size=latent_resolution, in_channels=vae.config.latent_channels, out_channels=vae.config.latent_channels,\n",
    "    layers_per_block=2, block_out_channels=(256, 512, 768, 1024),\n",
    "    down_block_types=(\"CrossAttnDownBlock2D\", \"CrossAttnDownBlock2D\", \"CrossAttnDownBlock2D\", \"DownBlock2D\"),\n",
    "    up_block_types=(\"UpBlock2D\", \"CrossAttnUpBlock2D\", \"CrossAttnUpBlock2D\", \"CrossAttnUpBlock2D\"),\n",
    "    cross_attention_dim=text_encoder.config.hidden_size,\n",
    ")\n",
    "unet_param_count = sum(p.numel() for p in original_unet.parameters() if p.requires_grad)\n",
    "accelerator.print(f\"UNet initialized with {unet_param_count / 1e6:.2f}M parameters.\")\n",
    "ema_unet = EMAModel(original_unet.parameters(), model_cls=UNet2DConditionModel, model_config=original_unet.config)\n",
    "ema_unet.to(accelerator.device)\n",
    "vae.to(accelerator.device).eval().requires_grad_(False)\n",
    "text_encoder.to(accelerator.device).eval().requires_grad_(False)\n",
    "\n",
    "with torch.no_grad():\n",
    "    uncond_tokens = tokenizer([\"\"], padding=\"max_length\", max_length=tokenizer.model_max_length, return_tensors=\"pt\").input_ids\n",
    "    uncond_embeddings = text_encoder(uncond_tokens.to(accelerator.device))[0]\n",
    "\n",
    "if xformers.version:\n",
    "    try:\n",
    "        original_unet.enable_xformers_memory_efficient_attention()\n",
    "        accelerator.print(\"XFormers memory efficient attention enabled for UNet.\")\n",
    "    except Exception as e:\n",
    "        accelerator.print(f\"Could not enable xformers memory efficient attention: {e}\")\n",
    "else:\n",
    "    accelerator.print(\"XFormers not available or not installed. Skipping memory efficient attention.\")\n",
    "compiled_unet_object = None\n",
    "accelerator.print(\"Skipping UNet compilation to avoid potential CUDAGraph issues.\")\n",
    "unet_to_prepare = original_unet\n",
    "\n",
    "# --- Noise Scheduler ---\n",
    "noise_scheduler = DDPMScheduler(\n",
    "    num_train_timesteps=num_train_timesteps_scheduler,\n",
    "    beta_schedule=\"scaled_linear\",\n",
    "    prediction_type=\"v_prediction\"\n",
    ")\n",
    "\n",
    "# --- Optimizer ---\n",
    "optimizer = AdamW(\n",
    "    unet_to_prepare.parameters(), lr=learning_rate, betas=(adam_beta1, adam_beta2),\n",
    "    weight_decay=adam_weight_decay, eps=adam_epsilon,\n",
    ")\n",
    "\n",
    "# --- Learning Rate Scheduler ---\n",
    "def lr_lambda_cosine(current_step: int):\n",
    "    if num_warmup_steps > 0 and current_step < num_warmup_steps:\n",
    "        return float(current_step) / float(max(1, num_warmup_steps))\n",
    "    denominator = float(max(1, max_train_steps - num_warmup_steps))\n",
    "    progress = float(current_step - num_warmup_steps) / denominator\n",
    "    return max(0.0, 0.5 * (1.0 + math.cos(math.pi * progress)))\n",
    "lr_scheduler = LambdaLR(optimizer, lr_lambda_cosine)\n",
    "\n",
    "\n",
    "# --- Sistemo img (Real Data Path) ---\n",
    "image_transforms = transforms.Compose([\n",
    "    transforms.Resize((image_resolution, image_resolution), interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "    transforms.ToTensor(), # Converts to [0,1] range, float32\n",
    "    transforms.Normalize([0.5], [0.5]), # Normalize to [-1, 1]\n",
    "])\n",
    "def preprocess_function(examples, tokenizer_obj, image_transforms_fn, accelerator_ref):\n",
    "    prompts, pil_images = examples[\"txt\"], examples[\"jpg\"]\n",
    "    processed_images, valid_prompts = [], []\n",
    "    for idx, (img_pil, prompt_text) in enumerate(zip(pil_images, prompts)):\n",
    "        try:\n",
    "            if img_pil is None:\n",
    "                if accelerator_ref.is_main_process: accelerator_ref.print(f\"Warning: Found None image for prompt: {prompt_text}. Skipping.\")\n",
    "                continue\n",
    "            img = img_pil.convert(\"RGB\")\n",
    "            img = ImageOps.exif_transpose(img)\n",
    "            processed_images.append(image_transforms_fn(img))\n",
    "            valid_prompts.append(prompt_text)\n",
    "        except Exception as e:\n",
    "            if accelerator_ref.is_main_process:\n",
    "                accelerator_ref.print(f\"Warning: Skipping an image/prompt due to error: {e}. Prompt: '{prompt_text}'. Image index in batch: {idx}\")\n",
    "            continue\n",
    "    if not processed_images:\n",
    "        return None\n",
    "    text_inputs = tokenizer_obj(valid_prompts, padding=\"max_length\", max_length=tokenizer_obj.model_max_length, truncation=True, return_tensors=\"pt\")\n",
    "    return {\"pixel_values\": torch.stack(processed_images), \"input_ids\": text_inputs.input_ids}\n",
    "\n",
    "class StreamingImageTextDataset(IterableDataset):\n",
    "    def __init__(self, dataset_id_str, split, transform_fn, tokenizer_obj, image_transforms_fn, processing_chunk_size, accelerator_ref):\n",
    "        self.dataset = load_dataset(dataset_id_str, split=split, streaming=True)\n",
    "        self.transform_fn = transform_fn\n",
    "        self.tokenizer_obj = tokenizer_obj\n",
    "        self.image_transforms_fn = image_transforms_fn\n",
    "        self.processing_chunk_size = processing_chunk_size\n",
    "        self.accelerator_ref = accelerator_ref\n",
    "\n",
    "    def __iter__(self):\n",
    "        buffer = []\n",
    "        for example in self.dataset:\n",
    "            if example.get(\"jpg\") is not None and example.get(\"txt\") is not None:\n",
    "                buffer.append({\"jpg\": example[\"jpg\"], \"txt\": example[\"txt\"]})\n",
    "            else:\n",
    "                if self.accelerator_ref.is_main_process:\n",
    "                    self.accelerator_ref.print(f\"Warning: Skipping example due to missing 'jpg' or 'txt' field: {example.get('txt', 'N/A')}\")\n",
    "                continue\n",
    "            if len(buffer) == self.processing_chunk_size:\n",
    "                processed_batch = self.transform_fn(\n",
    "                    {\"jpg\": [i[\"jpg\"] for i in buffer], \"txt\": [i[\"txt\"] for i in buffer]},\n",
    "                    self.tokenizer_obj, self.image_transforms_fn, self.accelerator_ref\n",
    "                )\n",
    "                if processed_batch:\n",
    "                    for i in range(processed_batch[\"pixel_values\"].size(0)):\n",
    "                        yield {\"pixel_values\": processed_batch[\"pixel_values\"][i], \"input_ids\": processed_batch[\"input_ids\"][i]}\n",
    "                buffer = []\n",
    "        if buffer:\n",
    "            processed_batch = self.transform_fn(\n",
    "                {\"jpg\": [i[\"jpg\"] for i in buffer], \"txt\": [i[\"txt\"] for i in buffer]},\n",
    "                self.tokenizer_obj, self.image_transforms_fn, self.accelerator_ref\n",
    "            )\n",
    "            if processed_batch:\n",
    "                for i in range(processed_batch[\"pixel_values\"].size(0)):\n",
    "                    yield {\"pixel_values\": processed_batch[\"pixel_values\"][i], \"input_ids\": processed_batch[\"input_ids\"][i]}\n",
    "\n",
    "# --- DATALOADER SETUP (REAL OR DUMMY) ---\n",
    "transform_processing_chunk_size = 1 # Used only for real data\n",
    "num_dataloader_workers = 8\n",
    "\n",
    "if use_dummy_data:\n",
    "    accelerator.print(\"INFO: Using DUMMY DATA for training to test GPU throughput.\")\n",
    "    train_dataset = DummyIterableDataset(\n",
    "        image_res=image_resolution,\n",
    "        tokenizer_max_len=tokenizer.model_max_length, # Ensure tokenizer is loaded before this\n",
    "        pixel_dtype=torch.float32 # Matches output of image_transforms\n",
    "    )\n",
    "else:\n",
    "    accelerator.print(\"Setting up real dataset stream...\")\n",
    "    train_dataset = StreamingImageTextDataset(\n",
    "        dataset_id_str=dataset_id, split=\"train\",\n",
    "        transform_fn=preprocess_function,\n",
    "        tokenizer_obj=tokenizer,\n",
    "        image_transforms_fn=image_transforms,\n",
    "        processing_chunk_size=transform_processing_chunk_size,\n",
    "        accelerator_ref=accelerator\n",
    "    )\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, batch_size=train_batch_size,\n",
    "    num_workers=num_dataloader_workers,\n",
    "    pin_memory=True if num_dataloader_workers > 0 else False,\n",
    "    persistent_workers=True if num_dataloader_workers > 0 else False\n",
    ")\n",
    "\n",
    "unet, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
    "    unet_to_prepare, optimizer, train_dataloader, lr_scheduler\n",
    ")\n",
    "\n",
    "# --- Resume from Checkpoint ---\n",
    "global_step, start_epoch, resume_wandb_id = 0, 0, None\n",
    "if not use_dummy_data: # Checkpoints are usually for real data runs\n",
    "    try:\n",
    "        global_step, start_epoch, resume_wandb_id = load_latest_checkpoint_custom(\n",
    "            accelerator, accelerator.unwrap_model(unet), ema_unet, optimizer, lr_scheduler, checkpoint_dir_base\n",
    "        )\n",
    "        if global_step > 0:\n",
    "            accelerator.print(f\"Resumed training from step {global_step}, epoch {start_epoch}.\")\n",
    "        else:\n",
    "            accelerator.print(\"Starting training from scratch or no compatible checkpoint found.\")\n",
    "    except Exception as e:\n",
    "        accelerator.print(f\"Could not load checkpoint. Starting from scratch. Error: {e}\")\n",
    "        global_step, start_epoch, resume_wandb_id = 0, 0, None\n",
    "else:\n",
    "    accelerator.print(\"INFO: Dummy data run. Checkpoint loading skipped. Starting from step 0.\")\n",
    "\n",
    "\n",
    "# --- WandB Initialization ---\n",
    "if use_wandb and accelerator.is_main_process and not use_dummy_data: # Optionally skip wandb for dummy data\n",
    "    wandb_config = {\n",
    "        \"learning_rate\": learning_rate, \"train_batch_size\": train_batch_size,\n",
    "        \"gradient_accumulation_steps\": gradient_accumulation_steps,\n",
    "        \"effective_batch_size\": train_batch_size * accelerator.num_processes * gradient_accumulation_steps,\n",
    "        \"image_resolution\": image_resolution, \"unet_params_M\": unet_param_count / 1e6,\n",
    "        \"vae_model_id\": vae_model_id, \"text_encoder_model_id\": text_encoder_model_id,\n",
    "        \"dataset_id\": dataset_id, \"max_train_steps\": max_train_steps,\n",
    "        \"num_warmup_steps\": num_warmup_steps, \"mixed_precision\": mixed_precision,\n",
    "        \"torch_compile_mode\": \"reduce-overhead\" if compiled_unet_object is not None else \"None\",\n",
    "        \"v_prediction\": True, \"cfg_drop_probability\": cfg_drop_probability, \"seed\": seed,\n",
    "        \"using_dummy_data\": use_dummy_data\n",
    "    }\n",
    "    run_id_to_resume = resume_wandb_id if global_step > 0 and resume_wandb_id else None\n",
    "    accelerator.init_trackers(\n",
    "        project_name=wandb_project_name,\n",
    "        config=wandb_config,\n",
    "        init_kwargs={\"wandb\": {\"id\": run_id_to_resume, \"resume\": \"allow\", \"mode\": \"online\" if not use_dummy_data else \"disabled\"}}\n",
    "    )\n",
    "    if run_id_to_resume: accelerator.print(f\"Resuming WandB run with ID: {run_id_to_resume}\")\n",
    "    elif global_step > 0: accelerator.print(\"Warning: Resuming training but no wandb_run_id found in checkpoint for WandB.\")\n",
    "elif use_dummy_data and accelerator.is_main_process:\n",
    "    accelerator.print(\"INFO: Dummy data run. WandB logging is disabled or limited.\")\n",
    "\n",
    "\n",
    "# --- Training Loop ---\n",
    "accelerator.print(f\"Starting training. Target steps: {max_train_steps}. Current step: {global_step}. Warmup steps: {num_warmup_steps}.\")\n",
    "progress_bar = tqdm(initial=global_step, total=max_train_steps, desc=\"Training Steps\", disable=not accelerator.is_main_process)\n",
    "current_epoch_for_tracking = start_epoch\n",
    "unet_for_sampling_config = accelerator.unwrap_model(unet).config\n",
    "\n",
    "while global_step < max_train_steps:\n",
    "    unet.train()\n",
    "    for batch_idx, batch in enumerate(train_dataloader):\n",
    "        if global_step >= max_train_steps:\n",
    "            break\n",
    "\n",
    "        with accelerator.accumulate(unet):\n",
    "            # Data is expected to be on CPU from DataLoader, needs to be moved to GPU\n",
    "            # For VAE and TextEncoder, which are on accelerator.device\n",
    "            pixel_values = batch[\"pixel_values\"].to(accelerator.device)\n",
    "            input_ids = batch[\"input_ids\"].to(accelerator.device)\n",
    "            bsz = pixel_values.shape[0]\n",
    "\n",
    "            with torch.no_grad():\n",
    "                latents = vae.encode(pixel_values).latent_dist.sample() * vae.config.scaling_factor\n",
    "                encoder_hidden_states = text_encoder(input_ids)[0]\n",
    "\n",
    "            mask = torch.rand(bsz, device=accelerator.device) < cfg_drop_probability\n",
    "            if mask.any():\n",
    "                 encoder_hidden_states[mask] = uncond_embeddings.expand(mask.sum(), -1, -1)\n",
    "\n",
    "            noise = torch.randn_like(latents) # latents are on device\n",
    "            timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (bsz,), device=latents.device).long()\n",
    "            noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
    "            target_velocity = noise_scheduler.get_velocity(latents, noise, timesteps)\n",
    "\n",
    "            model_pred = unet(noisy_latents, timesteps, encoder_hidden_states).sample\n",
    "            loss = F.mse_loss(model_pred.float(), target_velocity.float(), reduction=\"mean\")\n",
    "\n",
    "            accelerator.backward(loss)\n",
    "            if accelerator.sync_gradients:\n",
    "                accelerator.clip_grad_norm_(unet.parameters(), max_grad_norm)\n",
    "            \n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        if accelerator.sync_gradients:\n",
    "            ema_unet.step(accelerator.unwrap_model(unet).parameters())\n",
    "            progress_bar.update(1)\n",
    "            global_step += 1\n",
    "            \n",
    "            # Logging (can be simplified or skipped for dummy data runs if desired)\n",
    "            if global_step % 100 == 0: # Log every 100 steps\n",
    "                current_lr = optimizer.param_groups[0]['lr']\n",
    "                log_data = {\"train_loss\": loss.item(), \"learning_rate\": current_lr, \"global_step\": global_step}\n",
    "                if use_wandb and not use_dummy_data : # Only log to wandb if it's enabled and not dummy data\n",
    "                    accelerator.log(log_data, step=global_step)\n",
    "                elif accelerator.is_main_process: # Print to console for dummy data or if wandb is off\n",
    "                    print(f\"Step: {global_step}, Loss: {loss.item():.4f}, LR: {current_lr:.2e}\")\n",
    "                progress_bar.set_postfix({\"loss\": f\"{loss.item():.4f}\", \"lr\": f\"{current_lr:.2e}\"})\n",
    "\n",
    "            # Save Checkpoint (usually skipped for dummy data runs)\n",
    "            if not use_dummy_data and global_step > 0 and global_step % save_every_steps == 0 and save_every_steps > 0:\n",
    "                if accelerator.is_main_process:\n",
    "                    save_checkpoint_custom(\n",
    "                        accelerator, accelerator.unwrap_model(unet), ema_unet,\n",
    "                        optimizer, lr_scheduler, global_step, current_epoch_for_tracking,\n",
    "                        checkpoint_dir_base, max_keep=max_keep_checkpoints\n",
    "                    )\n",
    "                    unet_ema_sample_model = UNet2DConditionModel.from_config(unet_for_sampling_config).to(accelerator.device)\n",
    "                    ema_unet.copy_to(unet_ema_sample_model.parameters())\n",
    "                    generate_samples(\n",
    "                        unet_ema_sample_model, vae, text_encoder, tokenizer,\n",
    "                        noise_scheduler, sampling_prompts, output_dir_base,\n",
    "                        global_step, accelerator.device, accelerator\n",
    "                    )\n",
    "                    del unet_ema_sample_model\n",
    "                    if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
    "            \n",
    "            if global_step >= max_train_steps:\n",
    "                break\n",
    "    \n",
    "    current_epoch_for_tracking +=1 # Increment epoch conceptually after iterating through dataloader\n",
    "    if global_step >= max_train_steps:\n",
    "        break\n",
    "\n",
    "progress_bar.close()\n",
    "accelerator.print(\"Training finished.\")\n",
    "\n",
    "# --- Salva modello finale (usually skipped for dummy data runs) ---\n",
    "if accelerator.is_main_process and not use_dummy_data:\n",
    "    accelerator.print(f\"Saving final model and components to {final_model_dir}\")\n",
    "    os.makedirs(final_model_dir, exist_ok=True)\n",
    "    unwrapped_unet = accelerator.unwrap_model(unet)\n",
    "    unwrapped_unet.save_pretrained(os.path.join(final_model_dir, \"unet\"))\n",
    "    final_ema_unet_model = UNet2DConditionModel.from_config(unwrapped_unet.config).to(accelerator.device)\n",
    "    ema_unet.copy_to(final_ema_unet_model.parameters())\n",
    "    final_ema_unet_model.save_pretrained(os.path.join(final_model_dir, \"ema_unet\"))\n",
    "    del final_ema_unet_model\n",
    "    vae.save_pretrained(os.path.join(final_model_dir, \"vae\"))\n",
    "    text_encoder.save_pretrained(os.path.join(final_model_dir, \"text_encoder\"))\n",
    "    tokenizer.save_pretrained(os.path.join(final_model_dir, \"tokenizer\"))\n",
    "    noise_scheduler.save_config(os.path.join(final_model_dir, \"scheduler\"))\n",
    "    accelerator.save_state(os.path.join(final_model_dir, \"accelerator_state\"))\n",
    "    torch.save(ema_unet.state_dict(), os.path.join(final_model_dir, \"ema_unet_final_state.pth\"))\n",
    "    final_custom_state = {\n",
    "        'global_step': global_step, 'epoch': current_epoch_for_tracking,\n",
    "        'wandb_run_id': wandb.run.id if use_wandb and wandb.run else None\n",
    "    }\n",
    "    torch.save(final_custom_state, os.path.join(final_model_dir, \"final_custom_training_state.pt\"))\n",
    "    if use_wandb and wandb.run:\n",
    "        try:\n",
    "            final_model_artifact = wandb.Artifact(\n",
    "                name=f\"{wandb_project_name.lower().replace(' ', '_')}-final_model\", type=\"model\",\n",
    "                description=f\"Final trained diffusion model components at step {global_step}.\",\n",
    "                metadata=wandb_config # wandb_config might not be fully defined if dummy_data was true and wandb init was skipped\n",
    "            )\n",
    "            final_model_artifact.add_dir(final_model_dir)\n",
    "            wandb.log_artifact(final_model_artifact)\n",
    "            accelerator.print(\"Final model saved as WandB artifact.\")\n",
    "        except Exception as e:\n",
    "            accelerator.print(f\"Failed to save model as WandB artifact: {e}\")\n",
    "elif accelerator.is_main_process and use_dummy_data:\n",
    "    accelerator.print(\"INFO: Dummy data run. Final model saving skipped.\")\n",
    "\n",
    "accelerator.end_training()\n",
    "accelerator.print(\"All components processed. Training script complete.\")\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
